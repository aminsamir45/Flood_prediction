{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  country iso3   gwno  year  geo_id  geolocation  level     adm1  \\\n",
      "0  109  Albania  ALB  339.0  2009     346  Ana E Malit      3  Shkoder   \n",
      "1  109  Albania  ALB  339.0  2009     351       Bushat      3  Shkoder   \n",
      "\n",
      "       adm2         adm3     location  historical hist_country disastertype  \\\n",
      "0  Shkodres  Ana E Malit  Ana E Malit           0          NaN        flood   \n",
      "1  Shkodres       Bushat       Bushat           0          NaN        flood   \n",
      "\n",
      "  disasterno   latitude  longitude  \n",
      "0  2009-0631  42.020948  19.418317  \n",
      "1  2009-0631  41.959294  19.514309  \n",
      "          Dis No  Year   Seq Glide Disaster Group Disaster Subgroup  \\\n",
      "0  1900-9002-CPV  1900  9002   NaN        Natural    Climatological   \n",
      "1  1900-9001-IND  1900  9001   NaN        Natural    Climatological   \n",
      "\n",
      "  Disaster Type Disaster Subtype Disaster Subsubtype Event Name  ...  \\\n",
      "0       Drought          Drought                 NaN        NaN  ...   \n",
      "1       Drought          Drought                 NaN        NaN  ...   \n",
      "\n",
      "  Reconstruction Costs, Adjusted ('000 US$) Insured Damages ('000 US$)  \\\n",
      "0                                       NaN                        NaN   \n",
      "1                                       NaN                        NaN   \n",
      "\n",
      "  Insured Damages, Adjusted ('000 US$) Total Damages ('000 US$)  \\\n",
      "0                                  NaN                      NaN   \n",
      "1                                  NaN                      NaN   \n",
      "\n",
      "  Total Damages, Adjusted ('000 US$)       CPI Adm Level Admin1 Code  \\\n",
      "0                                NaN  3.077091       NaN         NaN   \n",
      "1                                NaN  3.077091       NaN         NaN   \n",
      "\n",
      "  Admin2 Code Geo Locations  \n",
      "0         NaN           NaN  \n",
      "1         NaN           NaN  \n",
      "\n",
      "[2 rows x 50 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/slurm_tmp/22086136.0.0/ipykernel_59796/995183751.py:4: DtypeWarning: Columns (8,16,17,18,19,24,25,26,27,46,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  emdat = pd.read_csv('../../data/emdat_public_2022_09_21_query_uid-47Yzpr.csv', skiprows=[0,1,2,3,4,5])\n"
     ]
    }
   ],
   "source": [
    "#gdis data \n",
    "gdis = pd.read_csv('../../data/pend-gdis-1960-2018-disasterlocations.csv')\n",
    "#get emdat dataset\n",
    "emdat = pd.read_csv('../../data/emdat_public_2022_09_21_query_uid-47Yzpr.csv', skiprows=[0,1,2,3,4,5])\n",
    "print(gdis.head(2))\n",
    "print(emdat.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select certain columns from emdat and join with gdis \n",
    "emdat['disasterno'] = emdat['Dis No'].str[:-4] #format disasterno to merge  \n",
    "cols = ['disasterno', 'Year', 'Event Name', \n",
    "#         'Disaster Type', 'Disaster Subtype', \n",
    "#         'Region', 'Continent', #'Location',\n",
    "        'Start Year', 'Start Month', 'Start Day', \n",
    "        'End Year', 'End Month','End Day',  \n",
    "        \"Total Damages, Adjusted ('000 US$)\"] \n",
    "\n",
    "emdat = emdat[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join emdat and gdis \n",
    "gdis = pd.merge(emdat, gdis, on = 'disasterno', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (9924, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disasterno</th>\n",
       "      <th>Year</th>\n",
       "      <th>Event Name</th>\n",
       "      <th>Start Year</th>\n",
       "      <th>Start Month</th>\n",
       "      <th>Start Day</th>\n",
       "      <th>End Year</th>\n",
       "      <th>End Month</th>\n",
       "      <th>End Day</th>\n",
       "      <th>Total Damages, Adjusted ('000 US$)</th>\n",
       "      <th>...</th>\n",
       "      <th>level</th>\n",
       "      <th>adm1</th>\n",
       "      <th>adm2</th>\n",
       "      <th>adm3</th>\n",
       "      <th>location</th>\n",
       "      <th>historical</th>\n",
       "      <th>hist_country</th>\n",
       "      <th>disastertype</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-0631</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Shkoder</td>\n",
       "      <td>Shkodres</td>\n",
       "      <td>Ana E Malit</td>\n",
       "      <td>Ana E Malit</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood</td>\n",
       "      <td>42.020948</td>\n",
       "      <td>19.418317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-0146</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Cunene</td>\n",
       "      <td>Cuanhama</td>\n",
       "      <td>Onjiva</td>\n",
       "      <td>Onjiva</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood</td>\n",
       "      <td>-17.093484</td>\n",
       "      <td>15.665758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  disasterno    Year Event Name  Start Year  Start Month  Start Day  End Year  \\\n",
       "0  2009-0631  2009.0        NaN      2009.0         12.0       27.0    2010.0   \n",
       "2  2001-0146  2001.0        NaN      2001.0          4.0        2.0    2001.0   \n",
       "\n",
       "   End Month  End Day  Total Damages, Adjusted ('000 US$)  ... level     adm1  \\\n",
       "0        1.0      8.0                                 NaN  ...     3  Shkoder   \n",
       "2        4.0      9.0                                 NaN  ...     3   Cunene   \n",
       "\n",
       "       adm2         adm3     location  historical hist_country  disastertype  \\\n",
       "0  Shkodres  Ana E Malit  Ana E Malit           0          NaN         flood   \n",
       "2  Cuanhama       Onjiva       Onjiva           0          NaN         flood   \n",
       "\n",
       "    latitude  longitude  \n",
       "0  42.020948  19.418317  \n",
       "2 -17.093484  15.665758  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print \n",
    "gdis = gdis.drop_duplicates(subset=['id'])\n",
    "print('shape', gdis.shape)\n",
    "gdis.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of grid pairs 2852\n"
     ]
    }
   ],
   "source": [
    "#latitude range from -90 to 90, longitude range from -180 to 180 \n",
    "\n",
    "#convert lat and long into 1 degree grid: correspond to 100*100km \n",
    "gdis['lat_grid'] = np.digitize(np.array(gdis['latitude']),np.arange(-90,90,1))\n",
    "gdis['lon_grid'] = np.digitize(np.array(gdis['longitude']),np.arange(-180,180,1)) \n",
    "#compute the grid pair id \n",
    "gdis['grid_id'] = list(zip(gdis['lat_grid'],gdis['lon_grid']))\n",
    "print('total number of grid pairs', len(gdis.grid_id.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disasterno</th>\n",
       "      <th>Year</th>\n",
       "      <th>Event Name</th>\n",
       "      <th>Start Year</th>\n",
       "      <th>Start Month</th>\n",
       "      <th>Start Day</th>\n",
       "      <th>End Year</th>\n",
       "      <th>End Month</th>\n",
       "      <th>End Day</th>\n",
       "      <th>Total Damages, Adjusted ('000 US$)</th>\n",
       "      <th>...</th>\n",
       "      <th>adm3</th>\n",
       "      <th>location</th>\n",
       "      <th>historical</th>\n",
       "      <th>hist_country</th>\n",
       "      <th>disastertype</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lat_grid</th>\n",
       "      <th>lon_grid</th>\n",
       "      <th>grid_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-0631</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Ana E Malit</td>\n",
       "      <td>Ana E Malit</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood</td>\n",
       "      <td>42.020948</td>\n",
       "      <td>19.418317</td>\n",
       "      <td>133</td>\n",
       "      <td>200</td>\n",
       "      <td>(133, 200)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-0146</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Onjiva</td>\n",
       "      <td>Onjiva</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood</td>\n",
       "      <td>-17.093484</td>\n",
       "      <td>15.665758</td>\n",
       "      <td>73</td>\n",
       "      <td>196</td>\n",
       "      <td>(73, 196)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  disasterno    Year Event Name  Start Year  Start Month  Start Day  End Year  \\\n",
       "0  2009-0631  2009.0        NaN      2009.0         12.0       27.0    2010.0   \n",
       "2  2001-0146  2001.0        NaN      2001.0          4.0        2.0    2001.0   \n",
       "\n",
       "   End Month  End Day  Total Damages, Adjusted ('000 US$)  ...         adm3  \\\n",
       "0        1.0      8.0                                 NaN  ...  Ana E Malit   \n",
       "2        4.0      9.0                                 NaN  ...       Onjiva   \n",
       "\n",
       "      location historical  hist_country  disastertype   latitude  longitude  \\\n",
       "0  Ana E Malit          0           NaN         flood  42.020948  19.418317   \n",
       "2       Onjiva          0           NaN         flood -17.093484  15.665758   \n",
       "\n",
       "   lat_grid lon_grid     grid_id  \n",
       "0       133      200  (133, 200)  \n",
       "2        73      196   (73, 196)  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdis.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #count number of locations in each grid_id: ideally just 1 location per grid_id, if not, I can make grid finer. \n",
    "# gdis.groupby('grid_id').agg({'location':'nunique'}).sort_values(by='location').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the grid with most regions \n",
    "# gdis.loc[gdis['grid_id']==(113, 295)].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot function: change rows of info into tables \n",
    "def pivot(df_in, id_col='disastertype', id_list=['Flood']):\n",
    "    df = df_in.reset_index(drop = True)\n",
    "\n",
    "\n",
    "    for id in id_list:\n",
    "        #initialize columns\n",
    "        df[id+'_bin'] = 0\n",
    "        df[id+'_amt'] = 0\n",
    "        df[id+'_ct'] = 0\n",
    "        \n",
    "\n",
    "        df.loc[(df[id_col]==id), id+'_bin'] = 1\n",
    "        df.loc[(df[id_col]==id), id+'_amt'] = df[\"Total Damages, Adjusted ('000 US$)\"].astype(float)\n",
    "        df.loc[(df[id_col]==id), id+'_ct'] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flood', 'storm', 'earthquake', 'extreme temperature ', 'landslide', 'volcanic activity', 'drought', 'mass movement (dry)']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disasterno</th>\n",
       "      <th>Year</th>\n",
       "      <th>Event Name</th>\n",
       "      <th>Start Year</th>\n",
       "      <th>Start Month</th>\n",
       "      <th>Start Day</th>\n",
       "      <th>End Year</th>\n",
       "      <th>End Month</th>\n",
       "      <th>End Day</th>\n",
       "      <th>Total Damages, Adjusted ('000 US$)</th>\n",
       "      <th>...</th>\n",
       "      <th>landslide_ct</th>\n",
       "      <th>volcanic activity_bin</th>\n",
       "      <th>volcanic activity_amt</th>\n",
       "      <th>volcanic activity_ct</th>\n",
       "      <th>drought_bin</th>\n",
       "      <th>drought_amt</th>\n",
       "      <th>drought_ct</th>\n",
       "      <th>mass movement (dry)_bin</th>\n",
       "      <th>mass movement (dry)_amt</th>\n",
       "      <th>mass movement (dry)_ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-0631</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-0146</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-0092</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-0105</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1995-0082</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1422569.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9919</th>\n",
       "      <td>1960-0011</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>Manam</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9920</th>\n",
       "      <td>2009-9633</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9921</th>\n",
       "      <td>1990-9289</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9922</th>\n",
       "      <td>1969-9069</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73867.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73867.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9923</th>\n",
       "      <td>2015-0375</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Hurricane Erika</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>551973.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9924 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     disasterno    Year       Event Name  Start Year  Start Month  Start Day  \\\n",
       "0     2009-0631  2009.0              NaN      2009.0         12.0       27.0   \n",
       "1     2001-0146  2001.0              NaN      2001.0          4.0        2.0   \n",
       "2     2009-0092  2009.0              NaN      2009.0          3.0        1.0   \n",
       "3     2010-0105  2010.0              NaN      2010.0          3.0        1.0   \n",
       "4     1995-0082  1995.0              NaN      1995.0          5.0       15.0   \n",
       "...         ...     ...              ...         ...          ...        ...   \n",
       "9919  1960-0011  1960.0            Manam      1960.0          3.0       17.0   \n",
       "9920  2009-9633  2009.0              NaN      2009.0          NaN        NaN   \n",
       "9921  1990-9289  1990.0              NaN      1990.0          NaN        NaN   \n",
       "9922  1969-9069  1969.0              NaN      1969.0          NaN        NaN   \n",
       "9923  2015-0375  2015.0  Hurricane Erika      2015.0          8.0       27.0   \n",
       "\n",
       "      End Year  End Month  End Day  Total Damages, Adjusted ('000 US$)  ...  \\\n",
       "0       2010.0        1.0      8.0                                 NaN  ...   \n",
       "1       2001.0        4.0      9.0                                 NaN  ...   \n",
       "2       2009.0        4.0     16.0                                 NaN  ...   \n",
       "3       2010.0        3.0     17.0                                 NaN  ...   \n",
       "4       1995.0        5.0     15.0                           1422569.0  ...   \n",
       "...        ...        ...      ...                                 ...  ...   \n",
       "9919    1960.0        3.0     17.0                                 NaN  ...   \n",
       "9920    2010.0        NaN      NaN                                 NaN  ...   \n",
       "9921    1990.0        NaN      NaN                                 NaN  ...   \n",
       "9922    1971.0        NaN      NaN                             73867.0  ...   \n",
       "9923    2015.0        8.0     27.0                            551973.0  ...   \n",
       "\n",
       "     landslide_ct volcanic activity_bin volcanic activity_amt  \\\n",
       "0               0                     0                   0.0   \n",
       "1               0                     0                   0.0   \n",
       "2               0                     0                   0.0   \n",
       "3               0                     0                   0.0   \n",
       "4               0                     0                   0.0   \n",
       "...           ...                   ...                   ...   \n",
       "9919            0                     1                   NaN   \n",
       "9920            0                     0                   0.0   \n",
       "9921            0                     0                   0.0   \n",
       "9922            0                     0                   0.0   \n",
       "9923            0                     0                   0.0   \n",
       "\n",
       "      volcanic activity_ct  drought_bin  drought_amt drought_ct  \\\n",
       "0                        0            0          0.0          0   \n",
       "1                        0            0          0.0          0   \n",
       "2                        0            0          0.0          0   \n",
       "3                        0            0          0.0          0   \n",
       "4                        0            0          0.0          0   \n",
       "...                    ...          ...          ...        ...   \n",
       "9919                     1            0          0.0          0   \n",
       "9920                     0            1          NaN          1   \n",
       "9921                     0            1          NaN          1   \n",
       "9922                     0            1      73867.0          1   \n",
       "9923                     0            0          0.0          0   \n",
       "\n",
       "      mass movement (dry)_bin mass movement (dry)_amt mass movement (dry)_ct  \n",
       "0                           0                     0.0                      0  \n",
       "1                           0                     0.0                      0  \n",
       "2                           0                     0.0                      0  \n",
       "3                           0                     0.0                      0  \n",
       "4                           0                     0.0                      0  \n",
       "...                       ...                     ...                    ...  \n",
       "9919                        0                     0.0                      0  \n",
       "9920                        0                     0.0                      0  \n",
       "9921                        0                     0.0                      0  \n",
       "9922                        0                     0.0                      0  \n",
       "9923                        0                     0.0                      0  \n",
       "\n",
       "[9924 rows x 54 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id_list= df_sub['Disaster Type'].unique()\n",
    "id_list= gdis['disastertype'].unique().tolist()\n",
    "print(id_list)\n",
    "df_pivot= pivot(gdis, id_col = 'disastertype', id_list = id_list)\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate columns by year\n",
    "def aggregate_yrly(df):\n",
    "    #aggregate count\n",
    "    col_ct = [col for col in df.columns if '_ct' in col]\n",
    "    df_ct = df.groupby(['grid_id','year'])[col_ct].agg('sum')\n",
    "    \n",
    "    #aggregate amount \n",
    "    col_amt = [col for col in df.columns if '_amt' in col]\n",
    "    df_amt = df.groupby(['grid_id','year'])[col_amt].agg('sum')\n",
    "    \n",
    "    #aggregate binary\n",
    "    col_bin = [col for col in df.columns if '_bin' in col]\n",
    "    df_bin= df.groupby(['grid_id','year'])[col_bin].agg('max')\n",
    "\n",
    "    #join\n",
    "    df1= pd.concat([df_amt, df_ct], axis=1)\n",
    "    df_out = pd.concat([df1, df_bin], axis=1)\n",
    "    return df_out.reset_index()\n",
    "df_yrly = aggregate_yrly(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_id</th>\n",
       "      <th>year</th>\n",
       "      <th>flood_amt</th>\n",
       "      <th>storm_amt</th>\n",
       "      <th>earthquake_amt</th>\n",
       "      <th>extreme temperature _amt</th>\n",
       "      <th>landslide_amt</th>\n",
       "      <th>volcanic activity_amt</th>\n",
       "      <th>drought_amt</th>\n",
       "      <th>mass movement (dry)_amt</th>\n",
       "      <th>...</th>\n",
       "      <th>drought_ct</th>\n",
       "      <th>mass movement (dry)_ct</th>\n",
       "      <th>flood_bin</th>\n",
       "      <th>storm_bin</th>\n",
       "      <th>earthquake_bin</th>\n",
       "      <th>extreme temperature _bin</th>\n",
       "      <th>landslide_bin</th>\n",
       "      <th>volcanic activity_bin</th>\n",
       "      <th>drought_bin</th>\n",
       "      <th>mass movement (dry)_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(37, 108)</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(42, 108)</td>\n",
       "      <td>2017</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(42, 111)</td>\n",
       "      <td>1991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(44, 107)</td>\n",
       "      <td>2007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(44, 107)</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8675</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>1998</td>\n",
       "      <td>5818.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8676</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2007</td>\n",
       "      <td>33655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8677</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8678</th>\n",
       "      <td>(158, 207)</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8679</th>\n",
       "      <td>(159, 45)</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8680 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         grid_id  year  flood_amt  storm_amt  earthquake_amt  \\\n",
       "0      (37, 108)  1990        0.0        0.0             0.0   \n",
       "1      (42, 108)  2017     2211.0        0.0             0.0   \n",
       "2      (42, 111)  1991        0.0        0.0             0.0   \n",
       "3      (44, 107)  2007        0.0        0.0             0.0   \n",
       "4      (44, 107)  2010        0.0        0.0             0.0   \n",
       "...          ...   ...        ...        ...             ...   \n",
       "8675  (157, 311)  1998     5818.0        0.0             0.0   \n",
       "8676  (157, 311)  2007    33655.0        0.0             0.0   \n",
       "8677  (157, 311)  2012        0.0        0.0             0.0   \n",
       "8678  (158, 207)  2005        0.0        0.0             0.0   \n",
       "8679   (159, 45)  2006        0.0        0.0             0.0   \n",
       "\n",
       "      extreme temperature _amt  landslide_amt  volcanic activity_amt  \\\n",
       "0                          0.0            0.0                    0.0   \n",
       "1                          0.0            0.0                    0.0   \n",
       "2                          0.0            0.0                    0.0   \n",
       "3                          0.0            0.0                    0.0   \n",
       "4                          0.0            0.0                    0.0   \n",
       "...                        ...            ...                    ...   \n",
       "8675                       0.0            0.0                    0.0   \n",
       "8676                       0.0            0.0                    0.0   \n",
       "8677                       0.0            0.0                    0.0   \n",
       "8678                       0.0            0.0                    0.0   \n",
       "8679                       0.0            0.0                    0.0   \n",
       "\n",
       "      drought_amt  mass movement (dry)_amt  ...  drought_ct  \\\n",
       "0             0.0                      0.0  ...           0   \n",
       "1             0.0                      0.0  ...           0   \n",
       "2             0.0                      0.0  ...           0   \n",
       "3             0.0                      0.0  ...           0   \n",
       "4             0.0                      0.0  ...           0   \n",
       "...           ...                      ...  ...         ...   \n",
       "8675          0.0                      0.0  ...           0   \n",
       "8676          0.0                      0.0  ...           0   \n",
       "8677          0.0                      0.0  ...           0   \n",
       "8678          0.0                      0.0  ...           0   \n",
       "8679          0.0                      0.0  ...           0   \n",
       "\n",
       "      mass movement (dry)_ct  flood_bin  storm_bin  earthquake_bin  \\\n",
       "0                          0          1          0               0   \n",
       "1                          0          1          0               0   \n",
       "2                          0          0          0               0   \n",
       "3                          0          0          0               1   \n",
       "4                          0          0          0               0   \n",
       "...                      ...        ...        ...             ...   \n",
       "8675                       0          1          0               0   \n",
       "8676                       0          1          0               0   \n",
       "8677                       0          0          0               0   \n",
       "8678                       0          1          0               0   \n",
       "8679                       0          1          0               0   \n",
       "\n",
       "      extreme temperature _bin  landslide_bin  volcanic activity_bin  \\\n",
       "0                            0              0                      0   \n",
       "1                            0              0                      0   \n",
       "2                            0              0                      1   \n",
       "3                            0              0                      0   \n",
       "4                            1              0                      0   \n",
       "...                        ...            ...                    ...   \n",
       "8675                         0              0                      0   \n",
       "8676                         0              0                      0   \n",
       "8677                         1              0                      0   \n",
       "8678                         0              0                      0   \n",
       "8679                         0              0                      0   \n",
       "\n",
       "      drought_bin  mass movement (dry)_bin  \n",
       "0               0                        0  \n",
       "1               0                        0  \n",
       "2               0                        0  \n",
       "3               0                        0  \n",
       "4               0                        0  \n",
       "...           ...                      ...  \n",
       "8675            0                        0  \n",
       "8676            0                        0  \n",
       "8677            0                        0  \n",
       "8678            0                        0  \n",
       "8679            0                        0  \n",
       "\n",
       "[8680 rows x 26 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yrly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flood_ct\n",
      "0           1032\n",
      "1            983\n",
      "2            361\n",
      "3            190\n",
      "4             81\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'flood_ct'}>]], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#count how many flood for each grid over all data \n",
    "#sum grid id with total flood incidents\n",
    "flood_ct = df_yrly.groupby('grid_id').agg({'flood_ct':'sum'}) \n",
    "print(flood_ct.value_counts().head(5))\n",
    "flood_ct.hist() #[['grid_id','flood_ct']]\n",
    "\n",
    "#1032 grids have no flood before, 983 have 1 flood only, 361 have 2 floods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flood_ct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grid_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(37, 108)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(42, 108)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(42, 111)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(44, 107)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(44, 349)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(156, 217)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(157, 157)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(157, 311)</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(158, 207)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(159, 45)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2852 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            flood_ct\n",
       "grid_id             \n",
       "(37, 108)          1\n",
       "(42, 108)          1\n",
       "(42, 111)          0\n",
       "(44, 107)          0\n",
       "(44, 349)          1\n",
       "...              ...\n",
       "(156, 217)         1\n",
       "(157, 157)         0\n",
       "(157, 311)         2\n",
       "(158, 207)         1\n",
       "(159, 45)          1\n",
       "\n",
       "[2852 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create labels for training NLP \n",
    "flood_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Y Master\n",
    "look up table for all the previous flood events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_col =  [col for col in df_yrly.columns if '_bin' in col]\n",
    "df_yrly_bin = df_yrly[['grid_id','year'] + bin_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get a list of disasters and flood_id \n",
    "# all_dis = gdis[['grid_id','Year','disastertype']]\n",
    "\n",
    "# #\n",
    "# flood = all_dis.loc[all_dis['disastertype']=='flood']\n",
    "# flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960.0 2018.0\n",
      "168268\n"
     ]
    }
   ],
   "source": [
    "# get a list of grid_ids \n",
    "grid_id = gdis['grid_id'].unique()\n",
    "# get a list of year information \n",
    "print(gdis.Year.min(), gdis.Year.max())\n",
    "year_id = np.arange(1960, 2019, 1) \n",
    "#create multi-index: each grid id, spanning over the years \n",
    "idd = pd.MultiIndex.from_product([grid_id, year_id],\n",
    "                           names=['grid_id', 'year'])\n",
    "\n",
    "#length should be |years| * |grid_ids| \n",
    "print(len(idd))\n",
    "#get dataframe \n",
    "idd = idd.to_frame().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master disaster targets for all years and all ids: \n",
    "#merge with df_yrly \n",
    "y_master = pd.merge(idd, df_yrly_bin, on=['grid_id','year'], how='left').fillna(0)\n",
    "\n",
    "#keep just the binary\n",
    "# y_master.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4007.0\n",
      "4274\n"
     ]
    }
   ],
   "source": [
    "#check: \n",
    "print(y_master['flood_bin'].sum()) #total number of binary flood targets \n",
    "\n",
    "#total number of flood incidents: \n",
    "print(gdis.loc[gdis['disastertype']=='flood'].shape[0])\n",
    "\n",
    "#the two numbers are slightly different, but that's because some country have more than 1 flood per year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_id</th>\n",
       "      <th>year</th>\n",
       "      <th>flood_bin</th>\n",
       "      <th>storm_bin</th>\n",
       "      <th>earthquake_bin</th>\n",
       "      <th>extreme temperature _bin</th>\n",
       "      <th>landslide_bin</th>\n",
       "      <th>volcanic activity_bin</th>\n",
       "      <th>drought_bin</th>\n",
       "      <th>mass movement (dry)_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(133, 200)</td>\n",
       "      <td>1960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(133, 200)</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(133, 200)</td>\n",
       "      <td>1962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(133, 200)</td>\n",
       "      <td>1963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(133, 200)</td>\n",
       "      <td>1964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168263</th>\n",
       "      <td>(99, 210)</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168264</th>\n",
       "      <td>(99, 210)</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168265</th>\n",
       "      <td>(99, 210)</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168266</th>\n",
       "      <td>(99, 210)</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168267</th>\n",
       "      <td>(99, 210)</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168268 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           grid_id  year  flood_bin  storm_bin  earthquake_bin  \\\n",
       "0       (133, 200)  1960        0.0        0.0             0.0   \n",
       "1       (133, 200)  1961        0.0        0.0             0.0   \n",
       "2       (133, 200)  1962        0.0        0.0             0.0   \n",
       "3       (133, 200)  1963        0.0        0.0             0.0   \n",
       "4       (133, 200)  1964        0.0        0.0             0.0   \n",
       "...            ...   ...        ...        ...             ...   \n",
       "168263   (99, 210)  2014        0.0        0.0             0.0   \n",
       "168264   (99, 210)  2015        0.0        0.0             0.0   \n",
       "168265   (99, 210)  2016        0.0        0.0             0.0   \n",
       "168266   (99, 210)  2017        0.0        0.0             0.0   \n",
       "168267   (99, 210)  2018        0.0        0.0             0.0   \n",
       "\n",
       "        extreme temperature _bin  landslide_bin  volcanic activity_bin  \\\n",
       "0                            0.0            0.0                    0.0   \n",
       "1                            0.0            0.0                    0.0   \n",
       "2                            0.0            0.0                    0.0   \n",
       "3                            0.0            0.0                    0.0   \n",
       "4                            0.0            0.0                    0.0   \n",
       "...                          ...            ...                    ...   \n",
       "168263                       0.0            0.0                    0.0   \n",
       "168264                       0.0            0.0                    0.0   \n",
       "168265                       0.0            0.0                    0.0   \n",
       "168266                       0.0            0.0                    0.0   \n",
       "168267                       0.0            0.0                    0.0   \n",
       "\n",
       "        drought_bin  mass movement (dry)_bin  \n",
       "0               0.0                      0.0  \n",
       "1               0.0                      0.0  \n",
       "2               0.0                      0.0  \n",
       "3               0.0                      0.0  \n",
       "4               0.0                      0.0  \n",
       "...             ...                      ...  \n",
       "168263          0.0                      0.0  \n",
       "168264          0.0                      0.0  \n",
       "168265          0.0                      0.0  \n",
       "168266          0.0                      0.0  \n",
       "168267          0.0                      0.0  \n",
       "\n",
       "[168268 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct next n year target -> look up this table \n",
    "y_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data to previously flooded regions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of grid_ids selected 818\n",
      "len of idd 48262\n"
     ]
    }
   ],
   "source": [
    "#step 1: filter xy_df to those grid_ids with previous frequent flooding history \n",
    "agg = df_yrly.groupby('grid_id').agg({'flood_bin':'sum'})\n",
    "grid_id_ls = agg.loc[agg['flood_bin']>=2].index.tolist()\n",
    "print('no of grid_ids selected', len(grid_id_ls))\n",
    "\n",
    "\n",
    "#step 2: interpolate years to record all years, fill with 0 without any flood using idd \n",
    "#create multi-index: each grid id, spanning over the years \n",
    "year_id = np.arange(1960, 2019, 1) \n",
    "idd = pd.MultiIndex.from_product([grid_id_ls, year_id],\n",
    "                           names=['grid_id', 'year'])\n",
    "\n",
    "#length should be |years| * |grid_ids| \n",
    "print('len of idd', len(idd))\n",
    "#get dataframe \n",
    "idd = idd.to_frame().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8680, 26)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yrly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attach NLP feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "print(pickle.format_version)\n",
    "\n",
    "#load processed nlp features: \n",
    "#note: you can load other nlp features too, they all start with 'nlp_*' \n",
    "file = open('../../data/nlp_cls_transfer.pkl', 'rb') \n",
    "# dump information to that file\n",
    "df_nlp = pickle.load(file)\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_id</th>\n",
       "      <th>location</th>\n",
       "      <th>txt</th>\n",
       "      <th>flood_ct_x</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(37, 108)</td>\n",
       "      <td>['Punta Arenas']</td>\n",
       "      <td>Located on the Brunswick Peninsula, Punta Aren...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.066456</td>\n",
       "      <td>0.414817</td>\n",
       "      <td>-0.122134</td>\n",
       "      <td>0.051715</td>\n",
       "      <td>-0.037439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285712</td>\n",
       "      <td>0.175260</td>\n",
       "      <td>0.066858</td>\n",
       "      <td>0.402230</td>\n",
       "      <td>0.307911</td>\n",
       "      <td>0.413373</td>\n",
       "      <td>-0.304808</td>\n",
       "      <td>0.332228</td>\n",
       "      <td>0.176272</td>\n",
       "      <td>-0.206565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(42, 108)</td>\n",
       "      <td>[\" O'Higgins\"]</td>\n",
       "      <td>In pre-Quaternary times extensive Nothofagus f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.092892</td>\n",
       "      <td>0.383235</td>\n",
       "      <td>-0.157205</td>\n",
       "      <td>0.075415</td>\n",
       "      <td>-0.010989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295375</td>\n",
       "      <td>0.147407</td>\n",
       "      <td>-0.000792</td>\n",
       "      <td>0.327370</td>\n",
       "      <td>0.290429</td>\n",
       "      <td>0.431107</td>\n",
       "      <td>-0.327519</td>\n",
       "      <td>0.183837</td>\n",
       "      <td>0.048896</td>\n",
       "      <td>-0.185709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(42, 111)</td>\n",
       "      <td>['Santa Cruz']</td>\n",
       "      <td>Santa Cruz is on the northern edge of Monterey...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.055714</td>\n",
       "      <td>0.408227</td>\n",
       "      <td>-0.124557</td>\n",
       "      <td>-0.072216</td>\n",
       "      <td>-0.014373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375935</td>\n",
       "      <td>0.161357</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>0.396205</td>\n",
       "      <td>0.309829</td>\n",
       "      <td>0.434621</td>\n",
       "      <td>-0.271017</td>\n",
       "      <td>0.377438</td>\n",
       "      <td>0.124515</td>\n",
       "      <td>-0.173165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(44, 107)</td>\n",
       "      <td>['Aysen region', 'Aisen del Gral. Carlos Ibañe...</td>\n",
       "      <td>The Aysén del General Carlos Ibáñez del Campo ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.024954</td>\n",
       "      <td>0.460049</td>\n",
       "      <td>-0.140097</td>\n",
       "      <td>0.070532</td>\n",
       "      <td>-0.088511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303552</td>\n",
       "      <td>0.173102</td>\n",
       "      <td>0.097518</td>\n",
       "      <td>0.477680</td>\n",
       "      <td>0.316651</td>\n",
       "      <td>0.445075</td>\n",
       "      <td>-0.281092</td>\n",
       "      <td>0.293919</td>\n",
       "      <td>0.158472</td>\n",
       "      <td>-0.205128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(44, 349)</td>\n",
       "      <td>['Gore', 'Invercargill']</td>\n",
       "      <td>Politically, Southland proper extends from Fio...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.203946</td>\n",
       "      <td>0.334446</td>\n",
       "      <td>-0.077132</td>\n",
       "      <td>-0.035596</td>\n",
       "      <td>-0.048420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354634</td>\n",
       "      <td>0.133703</td>\n",
       "      <td>0.101278</td>\n",
       "      <td>0.401057</td>\n",
       "      <td>0.406879</td>\n",
       "      <td>0.274729</td>\n",
       "      <td>-0.296987</td>\n",
       "      <td>0.280970</td>\n",
       "      <td>0.098923</td>\n",
       "      <td>-0.194855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     grid_id                                           location  \\\n",
       "0  (37, 108)                                   ['Punta Arenas']   \n",
       "1  (42, 108)                                     [\" O'Higgins\"]   \n",
       "2  (42, 111)                                     ['Santa Cruz']   \n",
       "3  (44, 107)  ['Aysen region', 'Aisen del Gral. Carlos Ibañe...   \n",
       "4  (44, 349)                           ['Gore', 'Invercargill']   \n",
       "\n",
       "                                                 txt  flood_ct_x  label  \\\n",
       "0  Located on the Brunswick Peninsula, Punta Aren...         1.0      0   \n",
       "1  In pre-Quaternary times extensive Nothofagus f...         1.0      0   \n",
       "2  Santa Cruz is on the northern edge of Monterey...         0.0      0   \n",
       "3  The Aysén del General Carlos Ibáñez del Campo ...         0.0      0   \n",
       "4  Politically, Southland proper extends from Fio...         1.0      0   \n",
       "\n",
       "          0         1         2         3         4  ...        22        23  \\\n",
       "0 -0.066456  0.414817 -0.122134  0.051715 -0.037439  ...  0.285712  0.175260   \n",
       "1 -0.092892  0.383235 -0.157205  0.075415 -0.010989  ...  0.295375  0.147407   \n",
       "2 -0.055714  0.408227 -0.124557 -0.072216 -0.014373  ...  0.375935  0.161357   \n",
       "3 -0.024954  0.460049 -0.140097  0.070532 -0.088511  ...  0.303552  0.173102   \n",
       "4 -0.203946  0.334446 -0.077132 -0.035596 -0.048420  ...  0.354634  0.133703   \n",
       "\n",
       "         24        25        26        27        28        29        30  \\\n",
       "0  0.066858  0.402230  0.307911  0.413373 -0.304808  0.332228  0.176272   \n",
       "1 -0.000792  0.327370  0.290429  0.431107 -0.327519  0.183837  0.048896   \n",
       "2  0.086125  0.396205  0.309829  0.434621 -0.271017  0.377438  0.124515   \n",
       "3  0.097518  0.477680  0.316651  0.445075 -0.281092  0.293919  0.158472   \n",
       "4  0.101278  0.401057  0.406879  0.274729 -0.296987  0.280970  0.098923   \n",
       "\n",
       "         31  \n",
       "0 -0.206565  \n",
       "1 -0.185709  \n",
       "2 -0.173165  \n",
       "3 -0.205128  \n",
       "4 -0.194855  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nlp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_id</th>\n",
       "      <th>year</th>\n",
       "      <th>flood_amt</th>\n",
       "      <th>storm_amt</th>\n",
       "      <th>earthquake_amt</th>\n",
       "      <th>extreme temperature _amt</th>\n",
       "      <th>landslide_amt</th>\n",
       "      <th>volcanic activity_amt</th>\n",
       "      <th>drought_amt</th>\n",
       "      <th>mass movement (dry)_amt</th>\n",
       "      <th>...</th>\n",
       "      <th>drought_ct</th>\n",
       "      <th>mass movement (dry)_ct</th>\n",
       "      <th>flood_bin</th>\n",
       "      <th>storm_bin</th>\n",
       "      <th>earthquake_bin</th>\n",
       "      <th>extreme temperature _bin</th>\n",
       "      <th>landslide_bin</th>\n",
       "      <th>volcanic activity_bin</th>\n",
       "      <th>drought_bin</th>\n",
       "      <th>mass movement (dry)_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(37, 108)</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(42, 108)</td>\n",
       "      <td>2017</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(42, 111)</td>\n",
       "      <td>1991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(44, 107)</td>\n",
       "      <td>2007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(44, 107)</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8675</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>1998</td>\n",
       "      <td>5818.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8676</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2007</td>\n",
       "      <td>33655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8677</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8678</th>\n",
       "      <td>(158, 207)</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8679</th>\n",
       "      <td>(159, 45)</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8680 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         grid_id  year  flood_amt  storm_amt  earthquake_amt  \\\n",
       "0      (37, 108)  1990        0.0        0.0             0.0   \n",
       "1      (42, 108)  2017     2211.0        0.0             0.0   \n",
       "2      (42, 111)  1991        0.0        0.0             0.0   \n",
       "3      (44, 107)  2007        0.0        0.0             0.0   \n",
       "4      (44, 107)  2010        0.0        0.0             0.0   \n",
       "...          ...   ...        ...        ...             ...   \n",
       "8675  (157, 311)  1998     5818.0        0.0             0.0   \n",
       "8676  (157, 311)  2007    33655.0        0.0             0.0   \n",
       "8677  (157, 311)  2012        0.0        0.0             0.0   \n",
       "8678  (158, 207)  2005        0.0        0.0             0.0   \n",
       "8679   (159, 45)  2006        0.0        0.0             0.0   \n",
       "\n",
       "      extreme temperature _amt  landslide_amt  volcanic activity_amt  \\\n",
       "0                          0.0            0.0                    0.0   \n",
       "1                          0.0            0.0                    0.0   \n",
       "2                          0.0            0.0                    0.0   \n",
       "3                          0.0            0.0                    0.0   \n",
       "4                          0.0            0.0                    0.0   \n",
       "...                        ...            ...                    ...   \n",
       "8675                       0.0            0.0                    0.0   \n",
       "8676                       0.0            0.0                    0.0   \n",
       "8677                       0.0            0.0                    0.0   \n",
       "8678                       0.0            0.0                    0.0   \n",
       "8679                       0.0            0.0                    0.0   \n",
       "\n",
       "      drought_amt  mass movement (dry)_amt  ...  drought_ct  \\\n",
       "0             0.0                      0.0  ...           0   \n",
       "1             0.0                      0.0  ...           0   \n",
       "2             0.0                      0.0  ...           0   \n",
       "3             0.0                      0.0  ...           0   \n",
       "4             0.0                      0.0  ...           0   \n",
       "...           ...                      ...  ...         ...   \n",
       "8675          0.0                      0.0  ...           0   \n",
       "8676          0.0                      0.0  ...           0   \n",
       "8677          0.0                      0.0  ...           0   \n",
       "8678          0.0                      0.0  ...           0   \n",
       "8679          0.0                      0.0  ...           0   \n",
       "\n",
       "      mass movement (dry)_ct  flood_bin  storm_bin  earthquake_bin  \\\n",
       "0                          0          1          0               0   \n",
       "1                          0          1          0               0   \n",
       "2                          0          0          0               0   \n",
       "3                          0          0          0               1   \n",
       "4                          0          0          0               0   \n",
       "...                      ...        ...        ...             ...   \n",
       "8675                       0          1          0               0   \n",
       "8676                       0          1          0               0   \n",
       "8677                       0          0          0               0   \n",
       "8678                       0          1          0               0   \n",
       "8679                       0          1          0               0   \n",
       "\n",
       "      extreme temperature _bin  landslide_bin  volcanic activity_bin  \\\n",
       "0                            0              0                      0   \n",
       "1                            0              0                      0   \n",
       "2                            0              0                      1   \n",
       "3                            0              0                      0   \n",
       "4                            1              0                      0   \n",
       "...                        ...            ...                    ...   \n",
       "8675                         0              0                      0   \n",
       "8676                         0              0                      0   \n",
       "8677                         1              0                      0   \n",
       "8678                         0              0                      0   \n",
       "8679                         0              0                      0   \n",
       "\n",
       "      drought_bin  mass movement (dry)_bin  \n",
       "0               0                        0  \n",
       "1               0                        0  \n",
       "2               0                        0  \n",
       "3               0                        0  \n",
       "4               0                        0  \n",
       "...           ...                      ...  \n",
       "8675            0                        0  \n",
       "8676            0                        0  \n",
       "8677            0                        0  \n",
       "8678            0                        0  \n",
       "8679            0                        0  \n",
       "\n",
       "[8680 rows x 26 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attach nlp to xy_df \n",
    "\n",
    "df_yrly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd\n",
    "import sys\n",
    "# setting path\n",
    "sys.path.append('../../src')\n",
    "\n",
    "#import model training module \n",
    "import models as m \n",
    "\n",
    "#split training and testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "# import shap\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attach target for a particular disease for next n years, using y_master  \n",
    "def attach_target(x_df, y_master, disaster, next_n): \n",
    "    y = y_master.copy()\n",
    "    #shift years\n",
    "    y['year'] = y['year'] - next_n\n",
    "    #keep for particular disaster \n",
    "    y = y[['grid_id','year',disaster+'_bin']]\n",
    "    y = y.rename(columns={disaster +'_bin': 'target_' + disaster + '_'+ str(next_n)})\n",
    "    xy_df = pd.merge(x_df, y, on = ['grid_id','year'], how='inner')\n",
    "    return xy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of xy_df 8464\n",
      "length of xy_df_sub 48262\n",
      "imbalance target_flood_1    0.015188\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#construct xy_df \n",
    "n_pred = 1\n",
    "\n",
    "xy_df = attach_target(df_yrly, y_master, 'flood', n_pred)\n",
    "print('length of xy_df', len(xy_df))\n",
    "\n",
    "#select to those id pass the filtering criteria \n",
    "xy_df_sub = xy_df.loc[xy_df['grid_id'].isin(grid_id_ls)]\n",
    "#interpolate missing years to have no flood \n",
    "xy_df_sub = pd.merge(idd, xy_df_sub, on=['grid_id','year'], how='left').fillna(0)\n",
    "\n",
    "print('length of xy_df_sub', len(xy_df_sub))\n",
    "\n",
    "print('imbalance', xy_df_sub.filter(regex='target').sum()/len(xy_df_sub))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total feature n (48262, 61)\n"
     ]
    }
   ],
   "source": [
    "#merge with NLP \n",
    "xy_df_sub1 = pd.merge(xy_df_sub, df_nlp.drop(['location','txt'], axis=1), on=['grid_id'], how='left') \n",
    "print('total feature n', xy_df_sub1.shape)\n",
    "#construct x,y train and test set \n",
    "x = xy_df_sub1.drop(xy_df_sub1.filter(regex='target').columns, axis=1)#drop target col \n",
    "x = x.select_dtypes(['number'])#drop index col\n",
    "y = xy_df_sub1.filter(regex='target') #filter to cols containing target \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_id</th>\n",
       "      <th>year</th>\n",
       "      <th>flood_amt</th>\n",
       "      <th>storm_amt</th>\n",
       "      <th>earthquake_amt</th>\n",
       "      <th>extreme temperature _amt</th>\n",
       "      <th>landslide_amt</th>\n",
       "      <th>volcanic activity_amt</th>\n",
       "      <th>drought_amt</th>\n",
       "      <th>mass movement (dry)_amt</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(45, 348)</td>\n",
       "      <td>1960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(45, 348)</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(45, 348)</td>\n",
       "      <td>1962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(45, 348)</td>\n",
       "      <td>1963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(45, 348)</td>\n",
       "      <td>1964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48257</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48258</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48259</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48260</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48261</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48262 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          grid_id  year  flood_amt  storm_amt  earthquake_amt  \\\n",
       "0       (45, 348)  1960        0.0        0.0             0.0   \n",
       "1       (45, 348)  1961        0.0        0.0             0.0   \n",
       "2       (45, 348)  1962        0.0        0.0             0.0   \n",
       "3       (45, 348)  1963        0.0        0.0             0.0   \n",
       "4       (45, 348)  1964        0.0        0.0             0.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "48257  (157, 311)  2014        0.0        0.0             0.0   \n",
       "48258  (157, 311)  2015        0.0        0.0             0.0   \n",
       "48259  (157, 311)  2016        0.0        0.0             0.0   \n",
       "48260  (157, 311)  2017        0.0        0.0             0.0   \n",
       "48261  (157, 311)  2018        0.0        0.0             0.0   \n",
       "\n",
       "       extreme temperature _amt  landslide_amt  volcanic activity_amt  \\\n",
       "0                           0.0            0.0                    0.0   \n",
       "1                           0.0            0.0                    0.0   \n",
       "2                           0.0            0.0                    0.0   \n",
       "3                           0.0            0.0                    0.0   \n",
       "4                           0.0            0.0                    0.0   \n",
       "...                         ...            ...                    ...   \n",
       "48257                       0.0            0.0                    0.0   \n",
       "48258                       0.0            0.0                    0.0   \n",
       "48259                       0.0            0.0                    0.0   \n",
       "48260                       0.0            0.0                    0.0   \n",
       "48261                       0.0            0.0                    0.0   \n",
       "\n",
       "       drought_amt  mass movement (dry)_amt  ...  22  23  24  25  26  27  28  \\\n",
       "0              0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "1              0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "2              0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "3              0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "4              0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "...            ...                      ...  ...  ..  ..  ..  ..  ..  ..  ..   \n",
       "48257          0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "48258          0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "48259          0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "48260          0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "48261          0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "\n",
       "       29  30  31  \n",
       "0     NaN NaN NaN  \n",
       "1     NaN NaN NaN  \n",
       "2     NaN NaN NaN  \n",
       "3     NaN NaN NaN  \n",
       "4     NaN NaN NaN  \n",
       "...    ..  ..  ..  \n",
       "48257 NaN NaN NaN  \n",
       "48258 NaN NaN NaN  \n",
       "48259 NaN NaN NaN  \n",
       "48260 NaN NaN NaN  \n",
       "48261 NaN NaN NaN  \n",
       "\n",
       "[48262 rows x 61 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(xy_df_sub, df_nlp.drop(['location','txt'], axis=1), on=['grid_id'], how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[37],\n",
       " [42],\n",
       " [42],\n",
       " [44],\n",
       " [44],\n",
       " [44],\n",
       " [45],\n",
       " [45],\n",
       " [45],\n",
       " [45],\n",
       " [45],\n",
       " [46],\n",
       " [47],\n",
       " [47],\n",
       " [47],\n",
       " [47],\n",
       " [48],\n",
       " [48],\n",
       " [48],\n",
       " [48],\n",
       " [48],\n",
       " [49],\n",
       " [49],\n",
       " [49],\n",
       " [49],\n",
       " [49],\n",
       " [49],\n",
       " [50],\n",
       " [50],\n",
       " [50],\n",
       " [50],\n",
       " [51],\n",
       " [51],\n",
       " [51],\n",
       " [51],\n",
       " [52],\n",
       " [52],\n",
       " [52],\n",
       " [52],\n",
       " [52],\n",
       " [52],\n",
       " [52],\n",
       " [52],\n",
       " [52],\n",
       " [53],\n",
       " [53],\n",
       " [53],\n",
       " [53],\n",
       " [53],\n",
       " [54],\n",
       " [54],\n",
       " [54],\n",
       " [54],\n",
       " [54],\n",
       " [54],\n",
       " [54],\n",
       " [54],\n",
       " [55],\n",
       " [55],\n",
       " [55],\n",
       " [55],\n",
       " [55],\n",
       " [55],\n",
       " [55],\n",
       " [56],\n",
       " [56],\n",
       " [56],\n",
       " [56],\n",
       " [56],\n",
       " [56],\n",
       " [56],\n",
       " [56],\n",
       " [56],\n",
       " [56],\n",
       " [56],\n",
       " [56],\n",
       " [57],\n",
       " [57],\n",
       " [57],\n",
       " [57],\n",
       " [57],\n",
       " [57],\n",
       " [57],\n",
       " [57],\n",
       " [57],\n",
       " [57],\n",
       " [57],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [58],\n",
       " [59],\n",
       " [59],\n",
       " [59],\n",
       " [59],\n",
       " [59],\n",
       " [59],\n",
       " [59],\n",
       " [59],\n",
       " [59],\n",
       " [60],\n",
       " [60],\n",
       " [60],\n",
       " [60],\n",
       " [60],\n",
       " [60],\n",
       " [60],\n",
       " [60],\n",
       " [60],\n",
       " [61],\n",
       " [61],\n",
       " [61],\n",
       " [61],\n",
       " [61],\n",
       " [61],\n",
       " [61],\n",
       " [61],\n",
       " [61],\n",
       " [61],\n",
       " [61],\n",
       " [61],\n",
       " [62],\n",
       " [62],\n",
       " [62],\n",
       " [62],\n",
       " [62],\n",
       " [62],\n",
       " [62],\n",
       " [62],\n",
       " [62],\n",
       " [62],\n",
       " [62],\n",
       " [63],\n",
       " [63],\n",
       " [63],\n",
       " [63],\n",
       " [63],\n",
       " [63],\n",
       " [63],\n",
       " [63],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [65],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [66],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [67],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [68],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [69],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [70],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [71],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [72],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [73],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [74],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [75],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [77],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [78],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [79],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [80],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [81],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [82],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [83],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [84],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [85],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [86],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [87],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [88],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [89],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [90],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [91],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [92],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [93],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [94],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [95],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [96],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [97],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [98],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [99],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " [100],\n",
       " ...]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = df_nlp['grid_id'].str.extractall('(\\d+)').astype(int).loc[:,0,:]\n",
    "second = df_nlp['grid_id'].str.extractall('(\\d+)').astype(int).loc[:,1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(37, 108)]\n",
       "1        [(42, 108)]\n",
       "2        [(42, 111)]\n",
       "3        [(44, 107)]\n",
       "4        [(44, 349)]\n",
       "            ...     \n",
       "2847    [(156, 217)]\n",
       "2848    [(157, 157)]\n",
       "2849    [(157, 311)]\n",
       "2850    [(158, 207)]\n",
       "2851     [(159, 45)]\n",
       "Name: grid_id, Length: 2852, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nlp['grid_id'].str.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['(37', '108)'],\n",
       " ['(42', '108)'],\n",
       " ['(42', '111)'],\n",
       " ['(44', '107)'],\n",
       " ['(44', '349)'],\n",
       " ['(44', '350)'],\n",
       " ['(45', '110)'],\n",
       " ['(45', '113)'],\n",
       " ['(45', '348)'],\n",
       " ['(45', '350)'],\n",
       " ['(45', '351)'],\n",
       " ['(46', '352)'],\n",
       " ['(47', '108)'],\n",
       " ['(47', '350)'],\n",
       " ['(47', '352)'],\n",
       " ['(47', '353)'],\n",
       " ['(48', '107)'],\n",
       " ['(48', '108)'],\n",
       " ['(48', '328)'],\n",
       " ['(48', '351)'],\n",
       " ['(48', '354)'],\n",
       " ['(49', '108)'],\n",
       " ['(49', '109)'],\n",
       " ['(49', '327)'],\n",
       " ['(49', '353)'],\n",
       " ['(49', '355)'],\n",
       " ['(49', '356)'],\n",
       " ['(50', '108)'],\n",
       " ['(50', '113)'],\n",
       " ['(50', '118)'],\n",
       " ['(50', '356)'],\n",
       " ['(51', '108)'],\n",
       " ['(51', '109)'],\n",
       " ['(51', '356)'],\n",
       " ['(51', '357)'],\n",
       " ['(52', '107)'],\n",
       " ['(52', '108)'],\n",
       " ['(52', '109)'],\n",
       " ['(52', '110)'],\n",
       " ['(52', '112)'],\n",
       " ['(52', '169)'],\n",
       " ['(52', '356)'],\n",
       " ['(52', '357)'],\n",
       " ['(52', '358)'],\n",
       " ['(53', '107)'],\n",
       " ['(53', '108)'],\n",
       " ['(53', '109)'],\n",
       " ['(53', '123)'],\n",
       " ['(53', '325)'],\n",
       " ['(54', '107)'],\n",
       " ['(54', '108)'],\n",
       " ['(54', '120)'],\n",
       " ['(54', '123)'],\n",
       " ['(54', '325)'],\n",
       " ['(54', '330)'],\n",
       " ['(54', '355)'],\n",
       " ['(54', '356)'],\n",
       " ['(55', '108)'],\n",
       " ['(55', '117)'],\n",
       " ['(55', '122)'],\n",
       " ['(55', '319)'],\n",
       " ['(55', '326)'],\n",
       " ['(55', '330)'],\n",
       " ['(55', '354)'],\n",
       " ['(56', '109)'],\n",
       " ['(56', '110)'],\n",
       " ['(56', '112)'],\n",
       " ['(56', '121)'],\n",
       " ['(56', '122)'],\n",
       " ['(56', '123)'],\n",
       " ['(56', '124)'],\n",
       " ['(56', '125)'],\n",
       " ['(56', '126)'],\n",
       " ['(56', '202)'],\n",
       " ['(56', '299)'],\n",
       " ['(56', '319)'],\n",
       " ['(57', '110)'],\n",
       " ['(57', '120)'],\n",
       " ['(57', '121)'],\n",
       " ['(57', '199)'],\n",
       " ['(57', '200)'],\n",
       " ['(57', '201)'],\n",
       " ['(57', '207)'],\n",
       " ['(57', '296)'],\n",
       " ['(57', '327)'],\n",
       " ['(57', '330)'],\n",
       " ['(57', '332)'],\n",
       " ['(58', '109)'],\n",
       " ['(58', '117)'],\n",
       " ['(58', '119)'],\n",
       " ['(58', '121)'],\n",
       " ['(58', '123)'],\n",
       " ['(58', '124)'],\n",
       " ['(58', '125)'],\n",
       " ['(58', '126)'],\n",
       " ['(58', '207)'],\n",
       " ['(58', '296)'],\n",
       " ['(58', '328)'],\n",
       " ['(58', '331)'],\n",
       " ['(58', '332)'],\n",
       " ['(59', '109)'],\n",
       " ['(59', '113)'],\n",
       " ['(59', '116)'],\n",
       " ['(59', '119)'],\n",
       " ['(59', '123)'],\n",
       " ['(59', '125)'],\n",
       " ['(59', '129)'],\n",
       " ['(59', '296)'],\n",
       " ['(59', '333)'],\n",
       " ['(60', '110)'],\n",
       " ['(60', '115)'],\n",
       " ['(60', '120)'],\n",
       " ['(60', '124)'],\n",
       " ['(60', '130)'],\n",
       " ['(60', '208)'],\n",
       " ['(60', '210)'],\n",
       " ['(60', '330)'],\n",
       " ['(60', '333)'],\n",
       " ['(61', '3)'],\n",
       " ['(61', '113)'],\n",
       " ['(61', '121)'],\n",
       " ['(61', '127)'],\n",
       " ['(61', '129)'],\n",
       " ['(61', '130)'],\n",
       " ['(61', '131)'],\n",
       " ['(61', '208)'],\n",
       " ['(61', '209)'],\n",
       " ['(61', '210)'],\n",
       " ['(61', '211)'],\n",
       " ['(61', '331)'],\n",
       " ['(62', '114)'],\n",
       " ['(62', '115)'],\n",
       " ['(62', '123)'],\n",
       " ['(62', '128)'],\n",
       " ['(62', '131)'],\n",
       " ['(62', '199)'],\n",
       " ['(62', '207)'],\n",
       " ['(62', '209)'],\n",
       " ['(62', '211)'],\n",
       " ['(62', '329)'],\n",
       " ['(62', '334)'],\n",
       " ['(63', '114)'],\n",
       " ['(63', '117)'],\n",
       " ['(63', '124)'],\n",
       " ['(63', '132)'],\n",
       " ['(63', '210)'],\n",
       " ['(63', '212)'],\n",
       " ['(63', '213)'],\n",
       " ['(63', '334)'],\n",
       " ['(64', '111)'],\n",
       " ['(64', '120)'],\n",
       " ['(64', '122)'],\n",
       " ['(64', '123)'],\n",
       " ['(64', '125)'],\n",
       " ['(64', '126)'],\n",
       " ['(64', '128)'],\n",
       " ['(64', '130)'],\n",
       " ['(64', '131)'],\n",
       " ['(64', '132)'],\n",
       " ['(64', '206)'],\n",
       " ['(64', '207)'],\n",
       " ['(64', '208)'],\n",
       " ['(64', '209)'],\n",
       " ['(64', '212)'],\n",
       " ['(64', '331)'],\n",
       " ['(65', '122)'],\n",
       " ['(65', '123)'],\n",
       " ['(65', '124)'],\n",
       " ['(65', '125)'],\n",
       " ['(65', '126)'],\n",
       " ['(65', '129)'],\n",
       " ['(65', '130)'],\n",
       " ['(65', '206)'],\n",
       " ['(65', '211)'],\n",
       " ['(65', '213)'],\n",
       " ['(65', '214)'],\n",
       " ['(65', '226)'],\n",
       " ['(65', '303)'],\n",
       " ['(65', '323)'],\n",
       " ['(66', '116)'],\n",
       " ['(66', '123)'],\n",
       " ['(66', '129)'],\n",
       " ['(66', '132)'],\n",
       " ['(66', '198)'],\n",
       " ['(66', '206)'],\n",
       " ['(66', '213)'],\n",
       " ['(66', '214)'],\n",
       " ['(66', '215)'],\n",
       " ['(66', '225)'],\n",
       " ['(66', '226)'],\n",
       " ['(66', '227)'],\n",
       " ['(66', '331)'],\n",
       " ['(66', '333)'],\n",
       " ['(67', '38)'],\n",
       " ['(67', '111)'],\n",
       " ['(67', '115)'],\n",
       " ['(67', '122)'],\n",
       " ['(67', '124)'],\n",
       " ['(67', '128)'],\n",
       " ['(67', '134)'],\n",
       " ['(67', '135)'],\n",
       " ['(67', '207)'],\n",
       " ['(67', '210)'],\n",
       " ['(67', '213)'],\n",
       " ['(67', '224)'],\n",
       " ['(67', '225)'],\n",
       " ['(67', '227)'],\n",
       " ['(67', '228)'],\n",
       " ['(67', '314)'],\n",
       " ['(67', '330)'],\n",
       " ['(67', '331)'],\n",
       " ['(67', '332)'],\n",
       " ['(68', '111)'],\n",
       " ['(68', '113)'],\n",
       " ['(68', '116)'],\n",
       " ['(68', '117)'],\n",
       " ['(68', '120)'],\n",
       " ['(68', '123)'],\n",
       " ['(68', '127)'],\n",
       " ['(68', '132)'],\n",
       " ['(68', '136)'],\n",
       " ['(68', '137)'],\n",
       " ['(68', '138)'],\n",
       " ['(68', '198)'],\n",
       " ['(68', '207)'],\n",
       " ['(68', '215)'],\n",
       " ['(68', '216)'],\n",
       " ['(68', '224)'],\n",
       " ['(68', '225)'],\n",
       " ['(68', '226)'],\n",
       " ['(68', '228)'],\n",
       " ['(68', '295)'],\n",
       " ['(68', '297)'],\n",
       " ['(68', '325)'],\n",
       " ['(68', '347)'],\n",
       " ['(69', '5)'],\n",
       " ['(69', '6)'],\n",
       " ['(69', '117)'],\n",
       " ['(69', '119)'],\n",
       " ['(69', '130)'],\n",
       " ['(69', '137)'],\n",
       " ['(69', '138)'],\n",
       " ['(69', '207)'],\n",
       " ['(69', '209)'],\n",
       " ['(69', '212)'],\n",
       " ['(69', '227)'],\n",
       " ['(69', '228)'],\n",
       " ['(69', '229)'],\n",
       " ['(69', '236)'],\n",
       " ['(69', '298)'],\n",
       " ['(69', '304)'],\n",
       " ['(69', '329)'],\n",
       " ['(69', '346)'],\n",
       " ['(70', '114)'],\n",
       " ['(70', '116)'],\n",
       " ['(70', '121)'],\n",
       " ['(70', '126)'],\n",
       " ['(70', '137)'],\n",
       " ['(70', '209)'],\n",
       " ['(70', '211)'],\n",
       " ['(70', '212)'],\n",
       " ['(70', '213)'],\n",
       " ['(70', '215)'],\n",
       " ['(70', '229)'],\n",
       " ['(70', '236)'],\n",
       " ['(70', '238)'],\n",
       " ['(70', '329)'],\n",
       " ['(70', '345)'],\n",
       " ['(70', '346)'],\n",
       " ['(70', '347)'],\n",
       " ['(71', '6)'],\n",
       " ['(71', '111)'],\n",
       " ['(71', '125)'],\n",
       " ['(71', '137)'],\n",
       " ['(71', '138)'],\n",
       " ['(71', '140)'],\n",
       " ['(71', '203)'],\n",
       " ['(71', '204)'],\n",
       " ['(71', '213)'],\n",
       " ['(71', '214)'],\n",
       " ['(71', '215)'],\n",
       " ['(71', '227)'],\n",
       " ['(71', '228)'],\n",
       " ['(71', '229)'],\n",
       " ['(71', '244)'],\n",
       " ['(71', '308)'],\n",
       " ['(71', '314)'],\n",
       " ['(71', '327)'],\n",
       " ['(71', '344)'],\n",
       " ['(71', '350)'],\n",
       " ['(71', '359)'],\n",
       " ['(72', '66)'],\n",
       " ['(72', '111)'],\n",
       " ['(72', '113)'],\n",
       " ['(72', '114)'],\n",
       " ['(72', '115)'],\n",
       " ['(72', '136)'],\n",
       " ['(72', '140)'],\n",
       " ['(72', '195)'],\n",
       " ['(72', '198)'],\n",
       " ['(72', '200)'],\n",
       " ['(72', '210)'],\n",
       " ['(72', '227)'],\n",
       " ['(72', '303)'],\n",
       " ['(72', '319)'],\n",
       " ['(72', '358)'],\n",
       " ['(72', '359)'],\n",
       " ['(73', '6)'],\n",
       " ['(73', '31)'],\n",
       " ['(73', '110)'],\n",
       " ['(73', '113)'],\n",
       " ['(73', '114)'],\n",
       " ['(73', '115)'],\n",
       " ['(73', '119)'],\n",
       " ['(73', '139)'],\n",
       " ['(73', '196)'],\n",
       " ['(73', '197)'],\n",
       " ['(73', '199)'],\n",
       " ['(73', '200)'],\n",
       " ['(73', '204)'],\n",
       " ['(73', '206)'],\n",
       " ['(73', '210)'],\n",
       " ['(73', '212)'],\n",
       " ['(73', '216)'],\n",
       " ['(73', '217)'],\n",
       " ['(73', '218)'],\n",
       " ['(73', '229)'],\n",
       " ['(73', '230)'],\n",
       " ['(73', '311)'],\n",
       " ['(73', '326)'],\n",
       " ['(73', '349)'],\n",
       " ['(73', '358)'],\n",
       " ['(73', '359)'],\n",
       " ['(74', '29)'],\n",
       " ['(74', '109)'],\n",
       " ['(74', '110)'],\n",
       " ['(74', '111)'],\n",
       " ['(74', '113)'],\n",
       " ['(74', '115)'],\n",
       " ['(74', '131)'],\n",
       " ['(74', '196)'],\n",
       " ['(74', '200)'],\n",
       " ['(74', '208)'],\n",
       " ['(74', '211)'],\n",
       " ['(74', '212)'],\n",
       " ['(74', '215)'],\n",
       " ['(74', '216)'],\n",
       " ['(74', '217)'],\n",
       " ['(74', '220)'],\n",
       " ['(74', '227)'],\n",
       " ['(74', '230)'],\n",
       " ['(74', '326)'],\n",
       " ['(74', '336)'],\n",
       " ['(74', '348)'],\n",
       " ['(74', '349)'],\n",
       " ['(74', '359)'],\n",
       " ['(74', '360)'],\n",
       " ['(75', '7)'],\n",
       " ['(75', '106)'],\n",
       " ['(75', '108)'],\n",
       " ['(75', '109)'],\n",
       " ['(75', '110)'],\n",
       " ['(75', '111)'],\n",
       " ['(75', '112)'],\n",
       " ['(75', '113)'],\n",
       " ['(75', '125)'],\n",
       " ['(75', '193)'],\n",
       " ['(75', '203)'],\n",
       " ['(75', '204)'],\n",
       " ['(75', '208)'],\n",
       " ['(75', '209)'],\n",
       " ['(75', '210)'],\n",
       " ['(75', '213)'],\n",
       " ['(75', '214)'],\n",
       " ['(75', '215)'],\n",
       " ['(75', '216)'],\n",
       " ['(75', '220)'],\n",
       " ['(75', '231)'],\n",
       " ['(75', '347)'],\n",
       " ['(75', '348)'],\n",
       " ['(75', '349)'],\n",
       " ['(76', '10)'],\n",
       " ['(76', '11)'],\n",
       " ['(76', '105)'],\n",
       " ['(76', '106)'],\n",
       " ['(76', '108)'],\n",
       " ['(76', '109)'],\n",
       " ['(76', '110)'],\n",
       " ['(76', '111)'],\n",
       " ['(76', '112)'],\n",
       " ['(76', '113)'],\n",
       " ['(76', '194)'],\n",
       " ['(76', '204)'],\n",
       " ['(76', '207)'],\n",
       " ['(76', '209)'],\n",
       " ['(76', '215)'],\n",
       " ['(76', '216)'],\n",
       " ['(76', '220)'],\n",
       " ['(76', '230)'],\n",
       " ['(76', '313)'],\n",
       " ['(77', '8)'],\n",
       " ['(77', '9)'],\n",
       " ['(77', '105)'],\n",
       " ['(77', '106)'],\n",
       " ['(77', '108)'],\n",
       " ['(77', '109)'],\n",
       " ['(77', '110)'],\n",
       " ['(77', '114)'],\n",
       " ['(77', '115)'],\n",
       " ['(77', '195)'],\n",
       " ['(77', '203)'],\n",
       " ['(77', '206)'],\n",
       " ['(77', '212)'],\n",
       " ['(77', '215)'],\n",
       " ['(77', '229)'],\n",
       " ['(77', '230)'],\n",
       " ['(77', '348)'],\n",
       " ['(78', '104)'],\n",
       " ['(78', '105)'],\n",
       " ['(78', '106)'],\n",
       " ['(78', '108)'],\n",
       " ['(78', '109)'],\n",
       " ['(78', '139)'],\n",
       " ['(78', '140)'],\n",
       " ['(78', '142)'],\n",
       " ['(78', '194)'],\n",
       " ['(78', '196)'],\n",
       " ['(78', '198)'],\n",
       " ['(78', '214)'],\n",
       " ['(78', '220)'],\n",
       " ['(78', '224)'],\n",
       " ['(78', '225)'],\n",
       " ['(78', '349)'],\n",
       " ['(79', '103)'],\n",
       " ['(79', '104)'],\n",
       " ['(79', '106)'],\n",
       " ['(79', '110)'],\n",
       " ['(79', '112)'],\n",
       " ['(79', '113)'],\n",
       " ['(79', '116)'],\n",
       " ['(79', '138)'],\n",
       " ['(79', '213)'],\n",
       " ['(79', '214)'],\n",
       " ['(79', '215)'],\n",
       " ['(79', '224)'],\n",
       " ['(79', '341)'],\n",
       " ['(80', '103)'],\n",
       " ['(80', '105)'],\n",
       " ['(80', '115)'],\n",
       " ['(80', '132)'],\n",
       " ['(80', '143)'],\n",
       " ['(80', '194)'],\n",
       " ['(80', '195)'],\n",
       " ['(80', '196)'],\n",
       " ['(80', '201)'],\n",
       " ['(80', '215)'],\n",
       " ['(80', '219)'],\n",
       " ['(80', '237)'],\n",
       " ['(80', '331)'],\n",
       " ['(80', '342)'],\n",
       " ['(80', '347)'],\n",
       " ['(81', '9)'],\n",
       " ['(81', '103)'],\n",
       " ['(81', '104)'],\n",
       " ['(81', '110)'],\n",
       " ['(81', '144)'],\n",
       " ['(81', '145)'],\n",
       " ['(81', '195)'],\n",
       " ['(81', '197)'],\n",
       " ['(81', '201)'],\n",
       " ['(81', '211)'],\n",
       " ['(81', '214)'],\n",
       " ['(81', '217)'],\n",
       " ['(81', '219)'],\n",
       " ['(81', '220)'],\n",
       " ['(81', '303)'],\n",
       " ['(81', '304)'],\n",
       " ['(81', '305)'],\n",
       " ['(81', '306)'],\n",
       " ['(81', '328)'],\n",
       " ['(81', '340)'],\n",
       " ['(81', '341)'],\n",
       " ['(81', '360)'],\n",
       " ['(82', '8)'],\n",
       " ['(82', '102)'],\n",
       " ['(82', '112)'],\n",
       " ['(82', '143)'],\n",
       " ['(82', '145)'],\n",
       " ['(82', '146)'],\n",
       " ['(82', '194)'],\n",
       " ['(82', '207)'],\n",
       " ['(82', '211)'],\n",
       " ['(82', '214)'],\n",
       " ['(82', '217)'],\n",
       " ['(82', '292)'],\n",
       " ['(82', '293)'],\n",
       " ['(82', '294)'],\n",
       " ['(82', '295)'],\n",
       " ['(82', '296)'],\n",
       " ['(82', '297)'],\n",
       " ['(82', '298)'],\n",
       " ['(82', '299)'],\n",
       " ['(82', '301)'],\n",
       " ['(82', '302)'],\n",
       " ['(82', '303)'],\n",
       " ['(82', '304)'],\n",
       " ['(82', '305)'],\n",
       " ['(82', '306)'],\n",
       " ['(82', '307)'],\n",
       " ['(82', '328)'],\n",
       " ['(82', '329)'],\n",
       " ['(82', '337)'],\n",
       " ['(82', '342)'],\n",
       " ['(82', '360)'],\n",
       " ['(83', '103)'],\n",
       " ['(83', '104)'],\n",
       " ['(83', '138)'],\n",
       " ['(83', '144)'],\n",
       " ['(83', '145)'],\n",
       " ['(83', '196)'],\n",
       " ['(83', '212)'],\n",
       " ['(83', '217)'],\n",
       " ['(83', '219)'],\n",
       " ['(83', '287)'],\n",
       " ['(83', '288)'],\n",
       " ['(83', '289)'],\n",
       " ['(83', '290)'],\n",
       " ['(83', '291)'],\n",
       " ['(83', '292)'],\n",
       " ['(83', '293)'],\n",
       " ['(83', '294)'],\n",
       " ['(83', '295)'],\n",
       " ['(83', '308)'],\n",
       " ['(83', '325)'],\n",
       " ['(83', '337)'],\n",
       " ['(83', '359)'],\n",
       " ['(84', '101)'],\n",
       " ['(84', '102)'],\n",
       " ['(84', '104)'],\n",
       " ['(84', '194)'],\n",
       " ['(84', '204)'],\n",
       " ['(84', '207)'],\n",
       " ['(84', '209)'],\n",
       " ['(84', '216)'],\n",
       " ['(84', '217)'],\n",
       " ['(84', '220)'],\n",
       " ['(84', '287)'],\n",
       " ['(84', '288)'],\n",
       " ['(84', '289)'],\n",
       " ['(84', '291)'],\n",
       " ['(84', '324)'],\n",
       " ['(84', '325)'],\n",
       " ['(84', '326)'],\n",
       " ['(84', '327)'],\n",
       " ['(84', '328)'],\n",
       " ['(84', '336)'],\n",
       " ['(84', '337)'],\n",
       " ['(84', '357)'],\n",
       " ['(85', '100)'],\n",
       " ['(85', '102)'],\n",
       " ['(85', '103)'],\n",
       " ['(85', '109)'],\n",
       " ['(85', '135)'],\n",
       " ['(85', '141)'],\n",
       " ['(85', '144)'],\n",
       " ['(85', '145)'],\n",
       " ['(85', '194)'],\n",
       " ['(85', '195)'],\n",
       " ['(85', '199)'],\n",
       " ['(85', '213)'],\n",
       " ['(85', '216)'],\n",
       " ['(85', '219)'],\n",
       " ['(85', '286)'],\n",
       " ['(85', '300)'],\n",
       " ['(85', '324)'],\n",
       " ['(85', '325)'],\n",
       " ['(85', '326)'],\n",
       " ['(85', '330)'],\n",
       " ['(85', '332)'],\n",
       " ['(86', '100)'],\n",
       " ['(86', '101)'],\n",
       " ['(86', '102)'],\n",
       " ['(86', '106)'],\n",
       " ['(86', '107)'],\n",
       " ['(86', '116)'],\n",
       " ['(86', '192)'],\n",
       " ['(86', '193)'],\n",
       " ['(86', '196)'],\n",
       " ['(86', '199)'],\n",
       " ['(86', '202)'],\n",
       " ['(86', '210)'],\n",
       " ['(86', '211)'],\n",
       " ['(86', '217)'],\n",
       " ['(86', '218)'],\n",
       " ['(86', '219)'],\n",
       " ['(86', '220)'],\n",
       " ['(86', '236)'],\n",
       " ['(86', '284)'],\n",
       " ['(86', '300)'],\n",
       " ['(86', '301)'],\n",
       " ['(86', '310)'],\n",
       " ['(86', '319)'],\n",
       " ['(86', '320)'],\n",
       " ['(86', '322)'],\n",
       " ['(86', '324)'],\n",
       " ['(86', '325)'],\n",
       " ['(86', '333)'],\n",
       " ['(87', '100)'],\n",
       " ['(87', '101)'],\n",
       " ['(87', '102)'],\n",
       " ['(87', '108)'],\n",
       " ['(87', '110)'],\n",
       " ['(87', '139)'],\n",
       " ['(87', '142)'],\n",
       " ['(87', '192)'],\n",
       " ['(87', '209)'],\n",
       " ['(87', '210)'],\n",
       " ['(87', '211)'],\n",
       " ['(87', '212)'],\n",
       " ['(87', '213)'],\n",
       " ['(87', '214)'],\n",
       " ['(87', '217)'],\n",
       " ['(87', '218)'],\n",
       " ['(87', '219)'],\n",
       " ['(87', '220)'],\n",
       " ['(87', '283)'],\n",
       " ['(87', '284)'],\n",
       " ['(87', '285)'],\n",
       " ['(87', '296)'],\n",
       " ['(87', '299)'],\n",
       " ['(87', '300)'],\n",
       " ['(87', '301)'],\n",
       " ['(87', '302)'],\n",
       " ['(87', '303)'],\n",
       " ['(87', '307)'],\n",
       " ['(87', '309)'],\n",
       " ['(87', '316)'],\n",
       " ['(87', '317)'],\n",
       " ['(87', '318)'],\n",
       " ['(87', '319)'],\n",
       " ['(87', '322)'],\n",
       " ['(87', '324)'],\n",
       " ['(87', '333)'],\n",
       " ['(88', '100)'],\n",
       " ['(88', '101)'],\n",
       " ['(88', '102)'],\n",
       " ['(88', '120)'],\n",
       " ['(88', '199)'],\n",
       " ['(88', '209)'],\n",
       " ['(88', '210)'],\n",
       " ['(88', '211)'],\n",
       " ['(88', '213)'],\n",
       " ['(88', '214)'],\n",
       " ['(88', '216)'],\n",
       " ['(88', '283)'],\n",
       " ['(88', '296)'],\n",
       " ['(88', '300)'],\n",
       " ['(88', '301)'],\n",
       " ['(88', '313)'],\n",
       " ['(88', '315)'],\n",
       " ['(88', '320)'],\n",
       " ['(89', '100)'],\n",
       " ['(89', '101)'],\n",
       " ['(89', '102)'],\n",
       " ['(89', '109)'],\n",
       " ['(89', '190)'],\n",
       " ['(89', '191)'],\n",
       " ['(89', '209)'],\n",
       " ['(89', '210)'],\n",
       " ['(89', '211)'],\n",
       " ['(89', '212)'],\n",
       " ['(89', '215)'],\n",
       " ['(89', '216)'],\n",
       " ['(89', '217)'],\n",
       " ['(89', '220)'],\n",
       " ['(89', '221)'],\n",
       " ['(89', '280)'],\n",
       " ['(89', '281)'],\n",
       " ['(89', '282)'],\n",
       " ['(89', '283)'],\n",
       " ['(89', '284)'],\n",
       " ['(89', '294)'],\n",
       " ['(89', '297)'],\n",
       " ['(89', '300)'],\n",
       " ['(89', '301)'],\n",
       " ['(89', '302)'],\n",
       " ['(89', '303)'],\n",
       " ['(89', '312)'],\n",
       " ['(89', '314)'],\n",
       " ['(89', '317)'],\n",
       " ['(90', '100)'],\n",
       " ['(90', '101)'],\n",
       " ['(90', '102)'],\n",
       " ['(90', '103)'],\n",
       " ['(90', '104)'],\n",
       " ['(90', '197)'],\n",
       " ['(90', '199)'],\n",
       " ['(90', '209)'],\n",
       " ['(90', '210)'],\n",
       " ['(90', '211)'],\n",
       " ['(90', '212)'],\n",
       " ['(90', '215)'],\n",
       " ['(90', '216)'],\n",
       " ['(90', '218)'],\n",
       " ['(90', '219)'],\n",
       " ['(90', '221)'],\n",
       " ['(90', '281)'],\n",
       " ['(90', '292)'],\n",
       " ['(90', '300)'],\n",
       " ['(90', '308)'],\n",
       " ['(91', '101)'],\n",
       " ['(91', '102)'],\n",
       " ['(91', '104)'],\n",
       " ['(91', '105)'],\n",
       " ['(91', '107)'],\n",
       " ['(91', '127)'],\n",
       " ['(91', '191)'],\n",
       " ['(91', '199)'],\n",
       " ['(91', '206)'],\n",
       " ['(91', '211)'],\n",
       " ['(91', '213)'],\n",
       " ['(91', '214)'],\n",
       " ['(91', '215)'],\n",
       " ['(91', '216)'],\n",
       " ['(91', '222)'],\n",
       " ['(91', '224)'],\n",
       " ['(91', '279)'],\n",
       " ['(91', '280)'],\n",
       " ['(91', '281)'],\n",
       " ['(91', '282)'],\n",
       " ['(91', '285)'],\n",
       " ['(91', '290)'],\n",
       " ['(91', '302)'],\n",
       " ['(91', '303)'],\n",
       " ['(91', '304)'],\n",
       " ['(91', '308)'],\n",
       " ['(91', '309)'],\n",
       " ['(92', '102)'],\n",
       " ['(92', '103)'],\n",
       " ['(92', '104)'],\n",
       " ['(92', '105)'],\n",
       " ['(92', '198)'],\n",
       " ['(92', '210)'],\n",
       " ['(92', '215)'],\n",
       " ['(92', '216)'],\n",
       " ['(92', '219)'],\n",
       " ['(92', '221)'],\n",
       " ['(92', '223)'],\n",
       " ['(92', '224)'],\n",
       " ['(92', '225)'],\n",
       " ['(92', '278)'],\n",
       " ['(92', '280)'],\n",
       " ['(92', '284)'],\n",
       " ['(92', '291)'],\n",
       " ['(92', '297)'],\n",
       " ['(92', '301)'],\n",
       " ['(92', '305)'],\n",
       " ['(92', '306)'],\n",
       " ['(92', '308)'],\n",
       " ['(93', '102)'],\n",
       " ['(93', '103)'],\n",
       " ['(93', '104)'],\n",
       " ['(93', '105)'],\n",
       " ['(93', '114)'],\n",
       " ['(93', '119)'],\n",
       " ['(93', '122)'],\n",
       " ['(93', '212)'],\n",
       " ['(93', '213)'],\n",
       " ['(93', '214)'],\n",
       " ['(93', '222)'],\n",
       " ['(93', '226)'],\n",
       " ['(93', '277)'],\n",
       " ['(93', '278)'],\n",
       " ['(93', '280)'],\n",
       " ['(93', '281)'],\n",
       " ['(93', '283)'],\n",
       " ['(93', '284)'],\n",
       " ['(93', '294)'],\n",
       " ['(93', '306)'],\n",
       " ['(94', '104)'],\n",
       " ['(94', '105)'],\n",
       " ['(94', '108)'],\n",
       " ['(94', '115)'],\n",
       " ['(94', '123)'],\n",
       " ['(94', '125)'],\n",
       " ['(94', '128)'],\n",
       " ['(94', '192)'],\n",
       " ['(94', '200)'],\n",
       " ['(94', '209)'],\n",
       " ['(94', '212)'],\n",
       " ['(94', '215)'],\n",
       " ['(94', '216)'],\n",
       " ['(94', '219)'],\n",
       " ['(94', '221)'],\n",
       " ['(94', '222)'],\n",
       " ['(94', '278)'],\n",
       " ['(94', '279)'],\n",
       " ['(94', '280)'],\n",
       " ['(94', '282)'],\n",
       " ['(94', '283)'],\n",
       " ['(94', '284)'],\n",
       " ['(94', '306)'],\n",
       " ['(95', '104)'],\n",
       " ['(95', '105)'],\n",
       " ['(95', '106)'],\n",
       " ['(95', '107)'],\n",
       " ['(95', '111)'],\n",
       " ['(95', '121)'],\n",
       " ['(95', '190)'],\n",
       " ['(95', '191)'],\n",
       " ['(95', '192)'],\n",
       " ['(95', '196)'],\n",
       " ['(95', '197)'],\n",
       " ['(95', '198)'],\n",
       " ['(95', '199)'],\n",
       " ['(95', '212)'],\n",
       " ['(95', '224)'],\n",
       " ['(95', '226)'],\n",
       " ['(95', '276)'],\n",
       " ['(95', '277)'],\n",
       " ['(95', '282)'],\n",
       " ['(95', '283)'],\n",
       " ['(95', '284)'],\n",
       " ['(95', '296)'],\n",
       " ['(95', '307)'],\n",
       " ['(96', '103)'],\n",
       " ['(96', '104)'],\n",
       " ['(96', '105)'],\n",
       " ['(96', '107)'],\n",
       " ['(96', '109)'],\n",
       " ['(96', '176)'],\n",
       " ['(96', '178)'],\n",
       " ['(96', '179)'],\n",
       " ['(96', '180)'],\n",
       " ['(96', '181)'],\n",
       " ['(96', '186)'],\n",
       " ['(96', '187)'],\n",
       " ['(96', '188)'],\n",
       " ['(96', '189)'],\n",
       " ['(96', '190)'],\n",
       " ['(96', '191)'],\n",
       " ['(96', '200)'],\n",
       " ['(96', '201)'],\n",
       " ['(96', '217)'],\n",
       " ['(96', '223)'],\n",
       " ['(96', '225)'],\n",
       " ['(96', '226)'],\n",
       " ['(96', '276)'],\n",
       " ['(96', '277)'],\n",
       " ['(96', '278)'],\n",
       " ['(96', '281)'],\n",
       " ['(96', '283)'],\n",
       " ['(96', '297)'],\n",
       " ['(96', '298)'],\n",
       " ['(96', '300)'],\n",
       " ['(96', '302)'],\n",
       " ['(96', '306)'],\n",
       " ['(96', '343)'],\n",
       " ['(97', '103)'],\n",
       " ['(97', '104)'],\n",
       " ['(97', '105)'],\n",
       " ['(97', '107)'],\n",
       " ['(97', '109)'],\n",
       " ['(97', '110)'],\n",
       " ['(97', '117)'],\n",
       " ['(97', '122)'],\n",
       " ['(97', '123)'],\n",
       " ['(97', '169)'],\n",
       " ['(97', '170)'],\n",
       " ['(97', '172)'],\n",
       " ['(97', '179)'],\n",
       " ['(97', '180)'],\n",
       " ['(97', '182)'],\n",
       " ['(97', '183)'],\n",
       " ['(97', '184)'],\n",
       " ['(97', '186)'],\n",
       " ['(97', '187)'],\n",
       " ['(97', '188)'],\n",
       " ['(97', '194)'],\n",
       " ['(97', '197)'],\n",
       " ['(97', '198)'],\n",
       " ['(97', '214)'],\n",
       " ['(97', '217)'],\n",
       " ['(97', '218)'],\n",
       " ['(97', '221)'],\n",
       " ['(97', '223)'],\n",
       " ['(97', '224)'],\n",
       " ['(97', '228)'],\n",
       " ['(97', '229)'],\n",
       " ['(97', '261)'],\n",
       " ['(97', '280)'],\n",
       " ['(97', '281)'],\n",
       " ['(97', '282)'],\n",
       " ['(97', '297)'],\n",
       " ['(97', '298)'],\n",
       " ['(97', '303)'],\n",
       " ['(97', '305)'],\n",
       " ['(97', '306)'],\n",
       " ['(98', '100)'],\n",
       " ['(98', '103)'],\n",
       " ['(98', '104)'],\n",
       " ['(98', '105)'],\n",
       " ['(98', '106)'],\n",
       " ['(98', '107)'],\n",
       " ['(98', '108)'],\n",
       " ['(98', '112)'],\n",
       " ['(98', '168)'],\n",
       " ['(98', '169)'],\n",
       " ['(98', '171)'],\n",
       " ['(98', '172)'],\n",
       " ['(98', '179)'],\n",
       " ['(98', '182)'],\n",
       " ['(98', '183)'],\n",
       " ['(98', '184)'],\n",
       " ['(98', '185)'],\n",
       " ['(98', '186)'],\n",
       " ['(98', '187)'],\n",
       " ['(98', '199)'],\n",
       " ['(98', '200)'],\n",
       " ['(98', '213)'],\n",
       " ['(98', '215)'],\n",
       " ['(98', '219)'],\n",
       " ['(98', '260)'],\n",
       " ['(98', '261)'],\n",
       " ['(98', '262)'],\n",
       " ['(98', '280)'],\n",
       " ['(98', '281)'],\n",
       " ['(98', '303)'],\n",
       " ['(98', '304)'],\n",
       " ['(98', '305)'],\n",
       " ['(98', '306)'],\n",
       " ['(98', '307)'],\n",
       " ['(98', '315)'],\n",
       " ['(98', '332)'],\n",
       " ['(99', '98)'],\n",
       " ['(99', '99)'],\n",
       " ['(99', '100)'],\n",
       " ['(99', '101)'],\n",
       " ['(99', '103)'],\n",
       " ['(99', '105)'],\n",
       " ['(99', '106)'],\n",
       " ['(99', '108)'],\n",
       " ['(99', '111)'],\n",
       " ['(99', '116)'],\n",
       " ['(99', '167)'],\n",
       " ['(99', '170)'],\n",
       " ['(99', '172)'],\n",
       " ['(99', '183)'],\n",
       " ['(99', '184)'],\n",
       " ['(99', '187)'],\n",
       " ['(99', '196)'],\n",
       " ['(99', '197)'],\n",
       " ['(99', '199)'],\n",
       " ['(99', '207)'],\n",
       " ['(99', '209)'],\n",
       " ['(99', '210)'],\n",
       " ['(99', '215)'],\n",
       " ['(99', '219)'],\n",
       " ['(99', '221)'],\n",
       " ['(99', '224)'],\n",
       " ['(99', '229)'],\n",
       " ['(99', '258)'],\n",
       " ['(99', '261)'],\n",
       " ['(99', '262)'],\n",
       " ['(99', '279)'],\n",
       " ['(99', '280)'],\n",
       " ['(99', '281)'],\n",
       " ['(99', '286)'],\n",
       " ['(99', '303)'],\n",
       " ['(99', '304)'],\n",
       " ['(99', '305)'],\n",
       " ['(99', '306)'],\n",
       " ['(99', '307)'],\n",
       " ['(100', '95)'],\n",
       " ['(100', '96)'],\n",
       " ['(100', '97)'],\n",
       " ['(100', '98)'],\n",
       " ['(100', '100)'],\n",
       " ['(100', '101)'],\n",
       " ['(100', '102)'],\n",
       " ['(100', '105)'],\n",
       " ['(100', '108)'],\n",
       " ['(100', '112)'],\n",
       " ['(100', '113)'],\n",
       " ['(100', '117)'],\n",
       " ['(100', '167)'],\n",
       " ['(100', '171)'],\n",
       " ['(100', '178)'],\n",
       " ['(100', '180)'],\n",
       " ['(100', '181)'],\n",
       " ['(100', '182)'],\n",
       " ['(100', '186)'],\n",
       " ['(100', '188)'],\n",
       " ['(100', '192)'],\n",
       " ['(100', '193)'],\n",
       " ['(100', '194)'],\n",
       " ['(100', '196)'],\n",
       " ['(100', '199)'],\n",
       " ['(100', '210)'],\n",
       " ['(100', '217)'],\n",
       " ...)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(df_nlp['grid_id'].str.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 348)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_df_sub.grid_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_id</th>\n",
       "      <th>year</th>\n",
       "      <th>flood_amt</th>\n",
       "      <th>storm_amt</th>\n",
       "      <th>earthquake_amt</th>\n",
       "      <th>extreme temperature _amt</th>\n",
       "      <th>landslide_amt</th>\n",
       "      <th>volcanic activity_amt</th>\n",
       "      <th>drought_amt</th>\n",
       "      <th>mass movement (dry)_amt</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(45, 348)</td>\n",
       "      <td>1960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(45, 348)</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(45, 348)</td>\n",
       "      <td>1962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(45, 348)</td>\n",
       "      <td>1963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(45, 348)</td>\n",
       "      <td>1964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48257</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48258</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48259</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48260</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48261</th>\n",
       "      <td>(157, 311)</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48262 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          grid_id  year  flood_amt  storm_amt  earthquake_amt  \\\n",
       "0       (45, 348)  1960        0.0        0.0             0.0   \n",
       "1       (45, 348)  1961        0.0        0.0             0.0   \n",
       "2       (45, 348)  1962        0.0        0.0             0.0   \n",
       "3       (45, 348)  1963        0.0        0.0             0.0   \n",
       "4       (45, 348)  1964        0.0        0.0             0.0   \n",
       "...           ...   ...        ...        ...             ...   \n",
       "48257  (157, 311)  2014        0.0        0.0             0.0   \n",
       "48258  (157, 311)  2015        0.0        0.0             0.0   \n",
       "48259  (157, 311)  2016        0.0        0.0             0.0   \n",
       "48260  (157, 311)  2017        0.0        0.0             0.0   \n",
       "48261  (157, 311)  2018        0.0        0.0             0.0   \n",
       "\n",
       "       extreme temperature _amt  landslide_amt  volcanic activity_amt  \\\n",
       "0                           0.0            0.0                    0.0   \n",
       "1                           0.0            0.0                    0.0   \n",
       "2                           0.0            0.0                    0.0   \n",
       "3                           0.0            0.0                    0.0   \n",
       "4                           0.0            0.0                    0.0   \n",
       "...                         ...            ...                    ...   \n",
       "48257                       0.0            0.0                    0.0   \n",
       "48258                       0.0            0.0                    0.0   \n",
       "48259                       0.0            0.0                    0.0   \n",
       "48260                       0.0            0.0                    0.0   \n",
       "48261                       0.0            0.0                    0.0   \n",
       "\n",
       "       drought_amt  mass movement (dry)_amt  ...  22  23  24  25  26  27  28  \\\n",
       "0              0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "1              0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "2              0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "3              0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "4              0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "...            ...                      ...  ...  ..  ..  ..  ..  ..  ..  ..   \n",
       "48257          0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "48258          0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "48259          0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "48260          0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "48261          0.0                      0.0  ... NaN NaN NaN NaN NaN NaN NaN   \n",
       "\n",
       "       29  30  31  \n",
       "0     NaN NaN NaN  \n",
       "1     NaN NaN NaN  \n",
       "2     NaN NaN NaN  \n",
       "3     NaN NaN NaN  \n",
       "4     NaN NaN NaN  \n",
       "...    ..  ..  ..  \n",
       "48257 NaN NaN NaN  \n",
       "48258 NaN NaN NaN  \n",
       "48259 NaN NaN NaN  \n",
       "48260 NaN NaN NaN  \n",
       "48261 NaN NaN NaN  \n",
       "\n",
       "[48262 rows x 61 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_df_sub1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumb baseline AUC 0.5\n",
      "maximum f1 score, thres 0.49613724944320714 0.4\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.5 0.49613724944320714 0.9846674494094896 0.5 0.015332550590510394 0.0\n",
      "[[14257     0]\n",
      " [  222     0]]\n",
      "Dumb baseline 2 AUC 0.8074162399756843\n",
      "maximum f1 score, thres 0.6188569873840768 0.4\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.8074162399756843 0.6188569873840768 0.9438497133779957 0.8074162399756845 0.11634722749860305 0.6666666666666666\n",
      "[[13518   739]\n",
      " [   74   148]]\n"
     ]
    }
   ],
   "source": [
    "#try a simple baseline\n",
    "\n",
    "#try a dumb model predicting just zeros \n",
    "y_base = np.zeros(len(y_test)) \n",
    "print('Dumb baseline AUC', metrics.roc_auc_score(y_test,y_base))\n",
    "results['base1']=m.get_scores_clf(y_test,y_base)\n",
    "\n",
    "#try a less dumb model predicting outcome = current flood outcome \n",
    "y_base2 = x_test['flood_bin'] \n",
    "print('Dumb baseline 2 AUC', metrics.roc_auc_score(y_test,y_base2))\n",
    "results['base2']= m.get_scores_clf(y_test,y_base2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running log reg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1342, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 796, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 645, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 97, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/gridsan/czeng12/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#logreg model \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_pred, y_pred_prob \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_logreg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogreg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mget_scores_clf(y_test, y_pred_prob)\n",
      "File \u001b[0;32m~/flood/nbs/cynthia/../../src/models.py:56\u001b[0m, in \u001b[0;36mrun_logreg\u001b[0;34m(x_train, y_train, x_test)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#grid search\u001b[39;00m\n\u001b[1;32m     55\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator \u001b[38;5;241m=\u001b[39m est, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39mgs_metric, cv\u001b[38;5;241m=\u001b[39m cv_folds, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#training auc\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain AUC: \u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m.\u001b[39mroc_auc_score(y_train, gs\u001b[38;5;241m.\u001b[39mpredict_proba(x_train)[:,\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:73\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m as keyword args. From version 0.25 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing these as positional arguments will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult in an error\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(args_msg)),\n\u001b[1;32m     71\u001b[0m                   \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate({k: arg \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:765\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    763\u001b[0m refit_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1342\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1340\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1342\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m                           \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m                           \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1345\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:432\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:73\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m as keyword args. From version 0.25 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing these as positional arguments will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult in an error\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(args_msg)),\n\u001b[1;32m     71\u001b[0m                   \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate({k: arg \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:796\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 796\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m                \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m                \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m                \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m                \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m                \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m                \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[1;32m    805\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(y, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m'\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    806\u001b[0m                     ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:73\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m as keyword args. From version 0.25 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing these as positional arguments will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult in an error\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(args_msg)),\n\u001b[1;32m     71\u001b[0m                   \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate({k: arg \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:645\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    642\u001b[0m                          \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name))\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 645\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    649\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:97\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (allow_nan \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(X)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m     95\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(X)\u001b[38;5;241m.\u001b[39mall()):\n\u001b[1;32m     96\u001b[0m         type_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfinity\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_nan \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNaN, infinity\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 97\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m                 msg_err\u001b[38;5;241m.\u001b[39mformat\n\u001b[1;32m     99\u001b[0m                 (type_err,\n\u001b[1;32m    100\u001b[0m                  msg_dtype \u001b[38;5;28;01mif\u001b[39;00m msg_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    101\u001b[0m         )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "#logreg model \n",
    "y_pred, y_pred_prob = m.run_logreg(x_train, y_train, x_test)\n",
    "results['logreg'] = m.get_scores_clf(y_test, y_pred_prob)\n",
    "\n",
    "# print('Test AUC', metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running xgb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC:  0.9925761640165458\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum f1 score, thres 0.6348158775917638 0.4\n",
      "auc, f1, accu, accu_bl=  0.9608531165660996 0.6169862698052062 0.9528972995372609 0.7987073838234672\n",
      "[[13655   602]\n",
      " [   80   142]]\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_pred_prob = m.run_xgb(x_train, y_train, x_test)\n",
    "results['xgb'] = m.get_scores_clf(y_test, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC 0.9608531165660996\n"
     ]
    }
   ],
   "source": [
    "print('Test AUC', metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base1</th>\n",
       "      <th>base2</th>\n",
       "      <th>logreg</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.807416</td>\n",
       "      <td>0.552081</td>\n",
       "      <td>0.960853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.496137</td>\n",
       "      <td>0.618857</td>\n",
       "      <td>0.591265</td>\n",
       "      <td>0.634816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.984667</td>\n",
       "      <td>0.943850</td>\n",
       "      <td>0.969266</td>\n",
       "      <td>0.952897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.807416</td>\n",
       "      <td>0.614124</td>\n",
       "      <td>0.798707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      base1     base2    logreg       xgb\n",
       "0  0.500000  0.807416  0.552081  0.960853\n",
       "1  0.496137  0.618857  0.591265  0.634816\n",
       "2  0.984667  0.943850  0.969266  0.952897\n",
       "3  0.500000  0.807416  0.614124  0.798707"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results= pd.DataFrame(results)\n",
    "# results.to_clipboard(excel=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding climate data using google earth engine \n",
    "@Oscar you can take over from here :) \n",
    "- Goal: \n",
    "    - given a [grid_id, year] index, attach climate features obtained from GEE \n",
    "    - consider the following steps:\n",
    "        1. grid_id: -> corresponding lat, and longitude box (see above digitalization code) \n",
    "        2. lat, long box: -> compute e.g. the center lat, lon of that box  \n",
    "        3. lat, long, year -> try fetch features such as monthly temperature average\n",
    "        4. attach to the dataset above, re-run models \n",
    "    - Sources: \n",
    "        - https://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_MONTHLY \n",
    "\n",
    "- Alternative sources of climate data: \n",
    "    - https://psl.noaa.gov/data/gridded/ \n",
    "    - https://psl.noaa.gov/data/gridded/data.cmap.html \n",
    "    - https://confluence.ecmwf.int/display/CKB/ERA5-Land%3A+data+documentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
