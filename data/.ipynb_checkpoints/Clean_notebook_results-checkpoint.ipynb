{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae33676-fab7-4d01-8af6-9b718b1719c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d90979c-0d57-4c67-a046-898fc92a4207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/hc93gpc55qs79hz6r0bfm83m0000gn/T/ipykernel_76199/610470096.py:4: DtypeWarning: Columns (8,16,17,18,19,24,25,26,27,46,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  emdat = pd.read_csv('../../data/emdat_public_2022_09_21_query_uid-47Yzpr.csv', skiprows=[0,1,2,3,4,5])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>iso3</th>\n",
       "      <th>gwno</th>\n",
       "      <th>year</th>\n",
       "      <th>geo_id</th>\n",
       "      <th>geolocation</th>\n",
       "      <th>level</th>\n",
       "      <th>adm1</th>\n",
       "      <th>adm2</th>\n",
       "      <th>adm3</th>\n",
       "      <th>location</th>\n",
       "      <th>historical</th>\n",
       "      <th>hist_country</th>\n",
       "      <th>disastertype</th>\n",
       "      <th>disasterno</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109</td>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>339.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>346</td>\n",
       "      <td>Ana E Malit</td>\n",
       "      <td>3</td>\n",
       "      <td>Shkoder</td>\n",
       "      <td>Shkodres</td>\n",
       "      <td>Ana E Malit</td>\n",
       "      <td>Ana E Malit</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood</td>\n",
       "      <td>2009-0631</td>\n",
       "      <td>42.020948</td>\n",
       "      <td>19.418317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109</td>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>339.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>351</td>\n",
       "      <td>Bushat</td>\n",
       "      <td>3</td>\n",
       "      <td>Shkoder</td>\n",
       "      <td>Shkodres</td>\n",
       "      <td>Bushat</td>\n",
       "      <td>Bushat</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood</td>\n",
       "      <td>2009-0631</td>\n",
       "      <td>41.959294</td>\n",
       "      <td>19.514309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  country iso3   gwno  year  geo_id  geolocation  level     adm1  \\\n",
       "0  109  Albania  ALB  339.0  2009     346  Ana E Malit      3  Shkoder   \n",
       "1  109  Albania  ALB  339.0  2009     351       Bushat      3  Shkoder   \n",
       "\n",
       "       adm2         adm3     location  historical hist_country disastertype  \\\n",
       "0  Shkodres  Ana E Malit  Ana E Malit           0          NaN        flood   \n",
       "1  Shkodres       Bushat       Bushat           0          NaN        flood   \n",
       "\n",
       "  disasterno   latitude  longitude  \n",
       "0  2009-0631  42.020948  19.418317  \n",
       "1  2009-0631  41.959294  19.514309  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gdis data \n",
    "gdis = pd.read_csv('../../data/pend-gdis-1960-2018-disasterlocations.csv')\n",
    "#get emdat dataset\n",
    "emdat = pd.read_csv('../../data/emdat_public_2022_09_21_query_uid-47Yzpr.csv', skiprows=[0,1,2,3,4,5])\n",
    "gdis.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb3ce9f-ee6d-4ff7-bafa-e55ec4f2a951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (9924, 27)\n",
      "total number of grid pairs 2827\n",
      "lon range -178 180\n",
      "lat range -54 68\n"
     ]
    }
   ],
   "source": [
    "#select certain columns from emdat and join with gdis \n",
    "# we grab the disaster number and convert it into string format and grab everything except for the last 4 ( so only the dates)\n",
    "emdat['disasterno'] = emdat['Dis No'].str[:-4] #format disasterno to merge  \n",
    "\n",
    "#These are the columns that we want to keep  , the features below are left out!\n",
    "cols = ['disasterno', 'Year', 'Event Name', \n",
    "#         'Disaster Type', 'Disaster Subtype', \n",
    "#         'Region', 'Continent', #'Location',\n",
    "        'Start Year', 'Start Month', 'Start Day', \n",
    "        'End Year', 'End Month','End Day',  \n",
    "        \"Total Damages, Adjusted ('000 US$)\"] \n",
    "\n",
    "emdat = emdat[cols]\n",
    "\n",
    "#join emdat and gdis into one dataframe\n",
    "gdis = pd.merge(emdat, gdis, on = 'disasterno', how='right')\n",
    "\n",
    "gdis = gdis.drop_duplicates(subset=['id'])\n",
    "print('shape', gdis.shape)\n",
    "\n",
    "# New Grid IDs\n",
    "#new grid_id: round to integers \n",
    "gdis['lat_grid'] = gdis['latitude'].round().astype(int)\n",
    "gdis['lon_grid'] = gdis['longitude'].round().astype(int)\n",
    "gdis['grid_id'] = list(zip(gdis['lat_grid'],gdis['lon_grid'])) \n",
    "print('total number of grid pairs', len(gdis.grid_id.value_counts()))\n",
    "\n",
    "# check if they lie in range\n",
    "print('lon range', gdis['lon_grid'].min(), gdis['lon_grid'].max()) \n",
    "print('lat range', gdis['lat_grid'].min(), gdis['lat_grid'].max()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed24cfd2-4af9-4d22-a7c4-e217ee65a88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flood', 'storm', 'earthquake', 'extreme temperature ', 'landslide', 'volcanic activity', 'drought', 'mass movement (dry)']\n"
     ]
    }
   ],
   "source": [
    "# Construct X dataframe\n",
    "\n",
    "#pivot function: change rows of info into tables \n",
    "def pivot(df_in, id_col='disastertype', id_list=['Flood']):\n",
    "    #Drop and reset the index column\n",
    "    df = df_in.reset_index(drop = True)\n",
    "\n",
    "    # one set of disaster\n",
    "    for id in id_list:\n",
    "        #initialize columns\n",
    "        df[id+'_bin'] = 0\n",
    "        df[id+'_amt'] = 0\n",
    "        df[id+'_ct'] = 0\n",
    "        \n",
    "        \n",
    "        # so if the flood happens and we find it, we create these columns and get the flood costs\n",
    "        df.loc[(df[id_col]==id), id+'_bin'] = 1\n",
    "        df.loc[(df[id_col]==id), id+'_amt'] = df[\"Total Damages, Adjusted ('000 US$)\"].astype(float)\n",
    "        df.loc[(df[id_col]==id), id+'_ct'] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# id_list= df_sub['Disaster Type'].unique()\n",
    "id_list= gdis['disastertype'].unique().tolist()\n",
    "print(id_list)\n",
    "df_pivot= pivot(gdis, id_col = 'disastertype', id_list = id_list)\n",
    "\n",
    "\n",
    "#aggregate columns by year\n",
    "# We need to include the longitude and lattitude here \n",
    "def aggregate_yrly(df):\n",
    "    #aggregate count\n",
    "    col_ct = [col for col in df.columns if '_ct' in col]\n",
    "    df_ct = df.groupby(['grid_id','year'])[col_ct].agg('sum')\n",
    "    \n",
    "    #aggregate amount \n",
    "    col_amt = [col for col in df.columns if '_amt' in col]\n",
    "    df_amt = df.groupby(['grid_id','year'])[col_amt].agg('sum')\n",
    "    \n",
    "    #aggregate binary\n",
    "    col_bin = [col for col in df.columns if '_bin' in col]\n",
    "    df_bin= df.groupby(['grid_id','year'])[col_bin].agg('max')\n",
    "\n",
    "    #join\n",
    "    df1= pd.concat([df_amt, df_ct], axis=1)\n",
    "    df_out = pd.concat([df1, df_bin], axis=1)\n",
    "    \n",
    "    return df_out.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "df_yrly = aggregate_yrly(df_pivot)\n",
    "# df_yrly.to_csv('../../data/df_yrly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f464e1de-7758-4d05-9c7d-7b34e6f5747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7977\n"
     ]
    }
   ],
   "source": [
    "df_yrly = df_yrly.loc[df_yrly['year']>1979].copy().reset_index(drop=True)\n",
    "print(len(df_yrly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58faacf7-f26e-4f56-98ce-d570f71efeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960.0 2018.0\n",
      "113080\n",
      "no of grid_ids selected 791\n",
      "len of idd 31640\n"
     ]
    }
   ],
   "source": [
    "# Construct Y\n",
    "bin_col =  [col for col in df_yrly.columns if '_bin' in col]\n",
    "df_yrly_bin = df_yrly[['grid_id','year'] + bin_col]\n",
    "\n",
    "\n",
    "# get a list of grid_ids \n",
    "grid_id = gdis['grid_id'].unique()\n",
    "# get a list of year information \n",
    "print(gdis.Year.min(), gdis.Year.max())\n",
    "year_id = np.arange(1979, 2019, 1) \n",
    "#create multi-index: each grid id, spanning over the years \n",
    "idd = pd.MultiIndex.from_product([grid_id, year_id],\n",
    "                           names=['grid_id', 'year'])\n",
    "\n",
    "#length should be |years| * |grid_ids| \n",
    "print(len(idd))\n",
    "#get dataframe \n",
    "idd = idd.to_frame().reset_index(drop=True)\n",
    "\n",
    "#master disaster targets for all years and all ids: \n",
    "#merge with df_yrly \n",
    "y_master = pd.merge(idd, df_yrly_bin, on=['grid_id','year'], how='left').fillna(0)\n",
    "\n",
    "#keep just the binary\n",
    "# y_master.sum(axis=0)\n",
    "\n",
    "# Filter data to previously flooded region\n",
    "#step 1: filter xy_df to those grid_ids with previous frequent flooding history \n",
    "agg = df_yrly.groupby('grid_id').agg({'flood_bin':'sum'})\n",
    "grid_id_ls = agg.loc[agg['flood_bin']>=2].index.tolist()\n",
    "print('no of grid_ids selected', len(grid_id_ls))\n",
    "\n",
    "\n",
    "#step 2: interpolate years to record all years, fill with 0 without any flood using idd \n",
    "#create multi-index: each grid id, spanning over the years \n",
    "year_id = np.arange(1979, 2019, 1) \n",
    "idd = pd.MultiIndex.from_product([grid_id_ls, year_id],\n",
    "                           names=['grid_id', 'year'])\n",
    "\n",
    "#length should be |years| * |grid_ids| \n",
    "print('len of idd', len(idd))\n",
    "#get dataframe \n",
    "idd = idd.to_frame().reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "59096554-6927-400c-9b07-53b4c089ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# pwd\n",
    "import sys\n",
    "# setting path\n",
    "sys.path.append('../../src')\n",
    "\n",
    "#import model training module \n",
    "import models as m \n",
    "\n",
    "#split training and testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "# import shap\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle \n",
    "print(pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e3c7d46c-8c3d-44d8-bb53-7cc71b942d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_names = ['nlp_cls_transfer.pkl','nlp_avg.pkl','nlp_cls_finetune.pkl','nlp_cls_transfer.pkl','nlp_cls.pkl']\n",
    "\n",
    "\n",
    "#load processed nlp features: \n",
    "#note: you can load other nlp features too, they all start with 'nlp_*' \n",
    "file = open('../../data/nlp_cls_transfer.pkl', 'rb') \n",
    "# load file\n",
    "df_nlp = pickle.load(file)\n",
    "# close the file\n",
    "file.close()\n",
    "df_nlp.shape\n",
    "\n",
    "df_nlp = string_to_tuple(df_nlp, 'grid_id')\n",
    "\n",
    "df_nlp = df_nlp.drop(['location','txt','label','flood_ct_x'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6c325116-f8a9-4677-9ab5-690a85581669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "(21040, 25)\n",
      "running xgb...\n",
      "Train AUC:  0.6886016465616585\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.52599245585101 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6402855310507107 0.5042973522820507 0.7563761366156576 0.5629881091910904 0.10657797189646334 0.3268053855569155\n",
      "[[6554 1647]\n",
      " [ 550  267]]\n"
     ]
    }
   ],
   "source": [
    "# running the model \n",
    "\n",
    "#attach target for a particular disease for next n years, using y_master  \n",
    "#next_n is how we choose the next n-periods for the prediction target\n",
    "def attach_target(x_df, y_master, disaster, next_n): \n",
    "    y = y_master.copy()\n",
    "    #shift years\n",
    "    y['year'] = y['year'] - next_n\n",
    "    #keep for particular disaster \n",
    "    y = y[['grid_id','year',disaster+'_bin']]\n",
    "    # Rename into target\n",
    "    y = y.rename(columns={disaster +'_bin': 'target_' + disaster + '_'+ str(next_n)})\n",
    "    xy_df = pd.merge(x_df, y, on = ['grid_id','year'], how='inner')\n",
    "    return xy_df\n",
    "\n",
    "#construct x_df with various modalities: stat, nlp, era features \n",
    "\n",
    "#select to those id pass the filtering criteria \n",
    "x_df = df_yrly.loc[df_yrly['grid_id'].isin(grid_id_ls)]\n",
    "#interpolate missing years to have no flood \n",
    "x_df = pd.merge(idd, x_df, on=['grid_id','year'], how='left').fillna(0)\n",
    "#add back lat and long info \n",
    "#x_df[['lat', 'lon']] = pd.DataFrame(x_df['grid_id'].tolist(), index=x_df.index) Is this even necessary?\n",
    "\n",
    "print('length of x_df', len(x_df))\n",
    "\n",
    "\n",
    "#construct xy_df, depending on the n_pred target period \n",
    "n_pred = 2\n",
    "\n",
    "# x_df = x_df.loc[x_df['year']>=1979] #crop to after 1979 \n",
    "xy_df = attach_target(x_df, y_master, 'flood', n_pred)\n",
    "print('length of xy_df', len(xy_df))\n",
    "print('imbalance', xy_df.filter(regex='target').sum()/len(xy_df))\n",
    "\n",
    "#construct x,y train and test set \n",
    "x = xy_df.drop(xy_df.filter(regex='target').columns, axis=1)#drop target col \n",
    "x = x.select_dtypes(['number'])#drop index col\n",
    "y = xy_df.filter(regex='target') #filter to cols containing target \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "print(x_train.shape) #x_train contains stat features only \n",
    "\n",
    "\n",
    "y_pred, y_pred_prob = m.run_xgb(x_train, y_train, x_test)\n",
    "sc_xgb = m.get_scores_clf(y_test, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c901c8b4-614c-4fbd-92d3-11c12b2ba423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6402855310507107,\n",
       " 0.52599245585101,\n",
       " 0.7563761366156576,\n",
       " 0.5629881091910904,\n",
       " 0.10657797189646334,\n",
       " 0.3268053855569155)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_xgb # This is without attaching any of the other features in!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "562d4948-111d-4c08-9b9d-0cf56564d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "era_features = pd.read_csv('era_features.csv', index_col=0)\n",
    "era_features['grid_id'] = list(zip(era_features['lat'],era_features['long']))\n",
    "df_era = era_features.drop(['lat','long'],axis = 1)\n",
    "xy_df_sub_2 = pd.merge(df_era, xy_df, on=['grid_id','year'], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "886207ac-27c6-4d9e-89d1-994397f2d65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21040, 133)\n",
      "running xgb...\n",
      "Train AUC:  0.8505983512002224\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.542495211421765 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6446646429511164 0.5134967167232402 0.8112663561765359 0.5573506052117416 0.10694301893497406 0.24724602203182375\n",
      "[[7114 1087]\n",
      " [ 615  202]]\n"
     ]
    }
   ],
   "source": [
    "#construct x,y train and test set \n",
    "x = xy_df_sub_2.drop(xy_df_sub_2.filter(regex='target').columns, axis=1)#drop target col \n",
    "x = x.select_dtypes(['number'])#drop index col\n",
    "y = xy_df_sub_2.filter(regex='target') #filter to cols containing target \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "print(x_train.shape) #x_train contains stat features only \n",
    "\n",
    "\n",
    "y_pred, y_pred_prob = m.run_xgb(x_train, y_train, x_test)\n",
    "sc_xgb = m.get_scores_clf(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5445c655-9382-488e-b966-7413ed4dc7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6446646429511164,\n",
       " 0.542495211421765,\n",
       " 0.8112663561765359,\n",
       " 0.5573506052117416,\n",
       " 0.10694301893497406,\n",
       " 0.24724602203182375)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_xgb #Somehow ERA5 features gave worse performance? (which ERA features are we choosing?) (are we sure adding all of the ,helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "64dbb6d6-b8a2-4243-bdda-6b642c04d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_df_sub_3 =  pd.merge(df_nlp, xy_df_sub_2, on='grid_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "54abeff2-f0c1-4057-beac-a9f94eb0819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21040, 165)\n",
      "running xgb...\n",
      "Train AUC:  0.8505983512002224\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.542495211421765 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6446646429511164 0.5134967167232402 0.8112663561765359 0.5573506052117416 0.10694301893497406 0.24724602203182375\n",
      "[[7114 1087]\n",
      " [ 615  202]]\n"
     ]
    }
   ],
   "source": [
    "#construct x,y train and test set \n",
    "x = xy_df_sub_3.drop(xy_df_sub_3.filter(regex='target').columns, axis=1)#drop target col \n",
    "x = x.select_dtypes(['number'])#drop index col\n",
    "y = xy_df_sub_3.filter(regex='target') #filter to cols containing target \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "print(x_train.shape) #x_train contains stat features only \n",
    "\n",
    "\n",
    "y_pred, y_pred_prob = m.run_xgb(x_train, y_train, x_test)\n",
    "sc_xgb = m.get_scores_clf(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "54b74f98-27e7-4480-bbfb-493e354a7290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6446646429511164,\n",
       " 0.542495211421765,\n",
       " 0.8112663561765359,\n",
       " 0.5573506052117416,\n",
       " 0.10694301893497406,\n",
       " 0.24724602203182375)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "87beabd7-a255-450a-a754-4c9ef989b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nlp_modality(name = 'nlp_cls_transfer.pkl'):\n",
    "    \"\"\"\n",
    "    Helper function that let's us load the NLP featuer\n",
    "    \n",
    "    Input = String of the name of the NLP file \n",
    "    \"\"\"\n",
    "\n",
    "    #load processed nlp features: \n",
    "    #note: you can load other nlp features too, they all start with 'nlp_*' \n",
    "    file = open('../../data/'+name, 'rb') \n",
    "    # load file\n",
    "    df_nlp = pickle.load(file)\n",
    "    # close the file\n",
    "    file.close()\n",
    "    df_nlp.shape\n",
    "\n",
    "    #correct formatting for df_nlp \n",
    "    df_nlp = string_to_tuple(df_nlp, 'grid_id')\n",
    "    \n",
    "    #drop text, location and label columns \n",
    "    df_nlp = df_nlp.drop(['location','txt'], axis=1)\n",
    "    \n",
    "    return df_nlp\n",
    "\n",
    "def string_to_tuple(df, col): \n",
    "    try: \n",
    "        df[col] = df.apply(lambda row: eval(row[col]), axis=1) \n",
    "    except: \n",
    "        'error converting to tuple'\n",
    "    return df\n",
    "\n",
    "def attach_target(x_df, y_master, disaster, next_n): \n",
    "    y = y_master.copy()\n",
    "    #shift years\n",
    "    y['year'] = y['year'] - next_n\n",
    "    #keep for particular disaster \n",
    "    y = y[['grid_id','year',disaster+'_bin']]\n",
    "    # Rename into target\n",
    "    y = y.rename(columns={disaster +'_bin': 'target_' + disaster + '_'+ str(next_n)})\n",
    "    xy_df = pd.merge(x_df, y, on = ['grid_id','year'], how='inner')\n",
    "    return xy_df\n",
    "\n",
    "\n",
    "def data_production(df_yrly,n_pred,y_master,modality,nlp_name):\n",
    "    '''\n",
    "    Helper Functions to deal with modality\n",
    "    '''\n",
    "    \n",
    "    #select to those id pass the filtering criteria \n",
    "    x_df = df_yrly.loc[df_yrly['grid_id'].isin(grid_id_ls)]\n",
    "    #interpolate missing years to have no flood \n",
    "    x_df = pd.merge(idd, x_df, on=['grid_id','year'], how='left').fillna(0)\n",
    "    \n",
    "    print('length of x_df', len(x_df))\n",
    "\n",
    "    \n",
    "    xy_df_sub = attach_target(x_df, y_master, 'flood', n_pred)\n",
    "    print('length of xy_df', len(xy_df))\n",
    "\n",
    "    print('length of xy_df', len(xy_df_sub))\n",
    "    print('imbalance', xy_df_sub.filter(regex='target').sum()/len(xy_df_sub))\n",
    "    \n",
    "    name = \"statistical\"\n",
    "    for mod in modality:\n",
    "        name = name + \"_\" +mod\n",
    "        \n",
    "\n",
    "    if \"era5\" in modality:\n",
    "        # we have to attach at this step\n",
    "        era_features = pd.read_csv('era_features.csv', index_col=0)\n",
    "        era_features['grid_id'] = list(zip(era_features['lat'],era_features['long']))\n",
    "        df_era = era_features.drop(['lat','long'],axis = 1)\n",
    "        xy_df_sub = pd.merge(df_era, xy_df_sub, on=['grid_id','year'], how='right')\n",
    "    \n",
    "    #merge with NLP \n",
    "    if \"nlp\" in modality:\n",
    "        df_nlp = create_nlp_modality(nlp_name)\n",
    "        xy_df_sub = pd.merge(xy_df_sub, df_nlp,on=['grid_id'], how='left')\n",
    "        print('shape of xy_df with nlp features now included', xy_df_sub.shape)\n",
    "        name = name + nlp_name\n",
    "\n",
    "\n",
    "    #construct x,y train and test set \n",
    "    x = xy_df_sub.drop(xy_df_sub.filter(regex='target').columns, axis=1)#drop target col \n",
    "    x = x.select_dtypes(['number'])#drop index col\n",
    "    y = xy_df_sub.filter(regex='target') #filter to cols containing target \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "    print(x_train.shape) \n",
    "    \n",
    "    \n",
    "    #Gradient Boosting Running\n",
    "    results={}\n",
    "    y_pred, y_pred_prob = m.run_xgb(x_train, y_train, x_test)\n",
    "    results[name] = m.get_scores_clf(y_test, y_pred_prob)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2dd8bf39-3ca7-464c-b594-cbb18306d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_combi =  [ \n",
    "    [],\n",
    "    ['era5'],\n",
    "    ['nlp'],\n",
    "    ['era5','nlp']\n",
    "]\n",
    "nlp_names = ['nlp_cls_transfer.pkl','nlp_avg.pkl','nlp_cls_finetune.pkl','nlp_cls_transfer.pkl','nlp_cls.pkl']\n",
    "horizons = [i for i in range(1,5)]\n",
    "# horizons = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "93f1c474-34cf-434c-8a26-8f3e04a35ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5f790531-297c-4bb2-b673-b559357de7af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "(21594, 25)\n",
      "running xgb...\n",
      "Train AUC:  0.7016140105763371\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5497519370876384 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6451804244106573 0.5124804738130077 0.8022690437601296 0.5714710092390171 0.11443726592130024 0.2882147024504084\n",
      "[[7178 1220]\n",
      " [ 610  247]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "(21594, 133)\n",
      "running xgb...\n",
      "Train AUC:  0.865804017529067\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5576934105637437 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6566163027647578 0.5208983775810285 0.8314424635332253 0.5665905617912583 0.1149940196085657 0.24154025670945156\n",
      "[[7488  910]\n",
      " [ 650  207]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30849, 61)\n",
      "(21594, 59)\n",
      "running xgb...\n",
      "Train AUC:  0.7022028203125362\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5495789374616833 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6441384749327715 0.5108034932959024 0.8155591572123176 0.5636012964135763 0.11216499573217194 0.2543757292882147\n",
      "[[7330 1068]\n",
      " [ 639  218]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30849, 795)\n",
      "(21594, 793)\n",
      "running xgb...\n",
      "Train AUC:  0.7022028203125362\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5495789374616833 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6441384749327715 0.5108034932959024 0.8155591572123176 0.5636012964135763 0.11216499573217194 0.2543757292882147\n",
      "[[7330 1068]\n",
      " [ 639  218]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30849, 797)\n",
      "(21594, 795)\n",
      "running xgb...\n",
      "Train AUC:  0.7022028203125362\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5495789374616833 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6441384749327715 0.5108034932959024 0.8155591572123176 0.5636012964135763 0.11216499573217194 0.2543757292882147\n",
      "[[7330 1068]\n",
      " [ 639  218]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30849, 61)\n",
      "(21594, 59)\n",
      "running xgb...\n",
      "Train AUC:  0.7022028203125362\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5495789374616833 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6441384749327715 0.5108034932959024 0.8155591572123176 0.5636012964135763 0.11216499573217194 0.2543757292882147\n",
      "[[7330 1068]\n",
      " [ 639  218]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30849, 795)\n",
      "(21594, 793)\n",
      "running xgb...\n",
      "Train AUC:  0.7022028203125362\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5495789374616833 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6441384749327715 0.5108034932959024 0.8155591572123176 0.5636012964135763 0.11216499573217194 0.2543757292882147\n",
      "[[7330 1068]\n",
      " [ 639  218]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30849, 169)\n",
      "(21594, 167)\n",
      "running xgb...\n",
      "Train AUC:  0.865804017529067\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5576934105637437 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6566163027647578 0.5208983775810285 0.8314424635332253 0.5665905617912583 0.1149940196085657 0.24154025670945156\n",
      "[[7488  910]\n",
      " [ 650  207]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30849, 903)\n",
      "(21594, 901)\n",
      "running xgb...\n",
      "Train AUC:  0.865804017529067\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5576934105637437 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6566163027647578 0.5208983775810285 0.8314424635332253 0.5665905617912583 0.1149940196085657 0.24154025670945156\n",
      "[[7488  910]\n",
      " [ 650  207]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30849, 905)\n",
      "(21594, 903)\n",
      "running xgb...\n",
      "Train AUC:  0.865804017529067\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5576934105637437 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6566163027647578 0.5208983775810285 0.8314424635332253 0.5665905617912583 0.1149940196085657 0.24154025670945156\n",
      "[[7488  910]\n",
      " [ 650  207]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30849, 169)\n",
      "(21594, 167)\n",
      "running xgb...\n",
      "Train AUC:  0.865804017529067\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5576934105637437 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6566163027647578 0.5208983775810285 0.8314424635332253 0.5665905617912583 0.1149940196085657 0.24154025670945156\n",
      "[[7488  910]\n",
      " [ 650  207]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30849\n",
      "imbalance target_flood_1    0.090181\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30849, 903)\n",
      "(21594, 901)\n",
      "running xgb...\n",
      "Train AUC:  0.865804017529067\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5576934105637437 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6566163027647578 0.5208983775810285 0.8314424635332253 0.5665905617912583 0.1149940196085657 0.24154025670945156\n",
      "[[7488  910]\n",
      " [ 650  207]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "(21040, 25)\n",
      "running xgb...\n",
      "Train AUC:  0.6886016465616585\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.52599245585101 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6402855310507107 0.5042973522820507 0.7563761366156576 0.5629881091910904 0.10657797189646334 0.3268053855569155\n",
      "[[6554 1647]\n",
      " [ 550  267]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "(21040, 133)\n",
      "running xgb...\n",
      "Train AUC:  0.8505983512002224\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.542495211421765 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6446646429511164 0.5134967167232402 0.8112663561765359 0.5573506052117416 0.10694301893497406 0.24724602203182375\n",
      "[[7114 1087]\n",
      " [ 615  202]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30058, 61)\n",
      "(21040, 59)\n",
      "running xgb...\n",
      "Train AUC:  0.6865316284568349\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5286323569372017 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6409582704560165 0.5046121358688469 0.7599245952539365 0.565490117708128 0.1075405963288012 0.32802937576499386\n",
      "[[6585 1616]\n",
      " [ 549  268]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30058, 795)\n",
      "(21040, 793)\n",
      "running xgb...\n",
      "Train AUC:  0.6865316284568349\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5286323569372017 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6409582704560165 0.5046121358688469 0.7599245952539365 0.565490117708128 0.1075405963288012 0.32802937576499386\n",
      "[[6585 1616]\n",
      " [ 549  268]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30058, 797)\n",
      "(21040, 795)\n",
      "running xgb...\n",
      "Train AUC:  0.6865316284568349\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5286323569372017 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6409582704560165 0.5046121358688469 0.7599245952539365 0.565490117708128 0.1075405963288012 0.32802937576499386\n",
      "[[6585 1616]\n",
      " [ 549  268]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30058, 61)\n",
      "(21040, 59)\n",
      "running xgb...\n",
      "Train AUC:  0.6865316284568349\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5286323569372017 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6409582704560165 0.5046121358688469 0.7599245952539365 0.565490117708128 0.1075405963288012 0.32802937576499386\n",
      "[[6585 1616]\n",
      " [ 549  268]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30058, 795)\n",
      "(21040, 793)\n",
      "running xgb...\n",
      "Train AUC:  0.6865316284568349\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5286323569372017 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6409582704560165 0.5046121358688469 0.7599245952539365 0.565490117708128 0.1075405963288012 0.32802937576499386\n",
      "[[6585 1616]\n",
      " [ 549  268]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30058, 169)\n",
      "(21040, 167)\n",
      "running xgb...\n",
      "Train AUC:  0.8505983512002224\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.542495211421765 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6446646429511164 0.5134967167232402 0.8112663561765359 0.5573506052117416 0.10694301893497406 0.24724602203182375\n",
      "[[7114 1087]\n",
      " [ 615  202]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30058, 903)\n",
      "(21040, 901)\n",
      "running xgb...\n",
      "Train AUC:  0.8505983512002224\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.542495211421765 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6446646429511164 0.5134967167232402 0.8112663561765359 0.5573506052117416 0.10694301893497406 0.24724602203182375\n",
      "[[7114 1087]\n",
      " [ 615  202]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30058, 905)\n",
      "(21040, 903)\n",
      "running xgb...\n",
      "Train AUC:  0.8505983512002224\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.542495211421765 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6446646429511164 0.5134967167232402 0.8112663561765359 0.5573506052117416 0.10694301893497406 0.24724602203182375\n",
      "[[7114 1087]\n",
      " [ 615  202]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30058, 169)\n",
      "(21040, 167)\n",
      "running xgb...\n",
      "Train AUC:  0.8505983512002224\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.542495211421765 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6446646429511164 0.5134967167232402 0.8112663561765359 0.5573506052117416 0.10694301893497406 0.24724602203182375\n",
      "[[7114 1087]\n",
      " [ 615  202]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 30058\n",
      "imbalance target_flood_2    0.091956\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (30058, 903)\n",
      "(21040, 901)\n",
      "running xgb...\n",
      "Train AUC:  0.8505983512002224\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.542495211421765 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6446646429511164 0.5134967167232402 0.8112663561765359 0.5573506052117416 0.10694301893497406 0.24724602203182375\n",
      "[[7114 1087]\n",
      " [ 615  202]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "(20486, 25)\n",
      "running xgb...\n",
      "Train AUC:  0.6739458901010108\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5463023618205333 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6540431871836236 0.49701513507038575 0.8179022890331398 0.5556359647211921 0.11242780291798295 0.2315035799522673\n",
      "[[6988  955]\n",
      " [ 644  194]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "(20486, 133)\n",
      "running xgb...\n",
      "Train AUC:  0.8655054777776433\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5458868013782258 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.641852209522682 0.5079931852331542 0.8157385263637399 0.5560410736761959 0.11244609691347582 0.2350835322195704\n",
      "[[6966  977]\n",
      " [ 641  197]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (29267, 61)\n",
      "(20486, 59)\n",
      "running xgb...\n",
      "Train AUC:  0.6759004681690197\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5463619424360109 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6538523134853733 0.4929655522918376 0.8174467600501082 0.5559178808918076 0.11251277318947836 0.23269689737470167\n",
      "[[6983  960]\n",
      " [ 643  195]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (29267, 795)\n",
      "(20486, 793)\n",
      "running xgb...\n",
      "Train AUC:  0.6759004681690197\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5463619424360109 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6538523134853733 0.4929655522918376 0.8174467600501082 0.5559178808918076 0.11251277318947836 0.23269689737470167\n",
      "[[6983  960]\n",
      " [ 643  195]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (29267, 797)\n",
      "(20486, 795)\n",
      "running xgb...\n",
      "Train AUC:  0.6759004681690197\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5463619424360109 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6538523134853733 0.4929655522918376 0.8174467600501082 0.5559178808918076 0.11251277318947836 0.23269689737470167\n",
      "[[6983  960]\n",
      " [ 643  195]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (29267, 61)\n",
      "(20486, 59)\n",
      "running xgb...\n",
      "Train AUC:  0.6759004681690197\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5463619424360109 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6538523134853733 0.4929655522918376 0.8174467600501082 0.5559178808918076 0.11251277318947836 0.23269689737470167\n",
      "[[6983  960]\n",
      " [ 643  195]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (29267, 795)\n",
      "(20486, 793)\n",
      "running xgb...\n",
      "Train AUC:  0.6759004681690197\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5463619424360109 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6538523134853733 0.4929655522918376 0.8174467600501082 0.5559178808918076 0.11251277318947836 0.23269689737470167\n",
      "[[6983  960]\n",
      " [ 643  195]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (29267, 169)\n",
      "(20486, 167)\n",
      "running xgb...\n",
      "Train AUC:  0.8655054777776433\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5458868013782258 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.641852209522682 0.5079931852331542 0.8157385263637399 0.5560410736761959 0.11244609691347582 0.2350835322195704\n",
      "[[6966  977]\n",
      " [ 641  197]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (29267, 903)\n",
      "(20486, 901)\n",
      "running xgb...\n",
      "Train AUC:  0.8655054777776433\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5458868013782258 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.641852209522682 0.5079931852331542 0.8157385263637399 0.5560410736761959 0.11244609691347582 0.2350835322195704\n",
      "[[6966  977]\n",
      " [ 641  197]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (29267, 905)\n",
      "(20486, 903)\n",
      "running xgb...\n",
      "Train AUC:  0.8655054777776433\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5458868013782258 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.641852209522682 0.5079931852331542 0.8157385263637399 0.5560410736761959 0.11244609691347582 0.2350835322195704\n",
      "[[6966  977]\n",
      " [ 641  197]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (29267, 169)\n",
      "(20486, 167)\n",
      "running xgb...\n",
      "Train AUC:  0.8655054777776433\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5458868013782258 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.641852209522682 0.5079931852331542 0.8157385263637399 0.5560410736761959 0.11244609691347582 0.2350835322195704\n",
      "[[6966  977]\n",
      " [ 641  197]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 29267\n",
      "imbalance target_flood_3    0.093792\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (29267, 903)\n",
      "(20486, 901)\n",
      "running xgb...\n",
      "Train AUC:  0.8655054777776433\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5458868013782258 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.641852209522682 0.5079931852331542 0.8157385263637399 0.5560410736761959 0.11244609691347582 0.2350835322195704\n",
      "[[6966  977]\n",
      " [ 641  197]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "(19933, 25)\n",
      "running xgb...\n",
      "Train AUC:  0.6794556200700642\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.5342600673192508 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6333469419584494 0.5012862681225309 0.7628467751375395 0.571974527406089 0.11175125557650166 0.33799237611181704\n",
      "[[6251 1505]\n",
      " [ 521  266]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "(19933, 133)\n",
      "running xgb...\n",
      "Train AUC:  0.8457655979417902\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.540482161156669 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6362358805053496 0.5064858984823083 0.8011237270279761 0.5582326557199149 0.10851852556543629 0.2604828462515883\n",
      "[[6639 1117]\n",
      " [ 582  205]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (28476, 61)\n",
      "(19933, 59)\n",
      "running xgb...\n",
      "Train AUC:  0.6795859784496009\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.53287461381206 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6324345033037504 0.5038002434442063 0.7643684888212572 0.5682457258978252 0.11047509281625503 0.3278271918678526\n",
      "[[6272 1484]\n",
      " [ 529  258]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (28476, 795)\n",
      "(19933, 793)\n",
      "running xgb...\n",
      "Train AUC:  0.6795859784496009\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.53287461381206 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6324345033037504 0.5038002434442063 0.7643684888212572 0.5682457258978252 0.11047509281625503 0.3278271918678526\n",
      "[[6272 1484]\n",
      " [ 529  258]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (28476, 797)\n",
      "(19933, 795)\n",
      "running xgb...\n",
      "Train AUC:  0.6795859784496009\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.53287461381206 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6324345033037504 0.5038002434442063 0.7643684888212572 0.5682457258978252 0.11047509281625503 0.3278271918678526\n",
      "[[6272 1484]\n",
      " [ 529  258]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (28476, 61)\n",
      "(19933, 59)\n",
      "running xgb...\n",
      "Train AUC:  0.6795859784496009\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.53287461381206 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6324345033037504 0.5038002434442063 0.7643684888212572 0.5682457258978252 0.11047509281625503 0.3278271918678526\n",
      "[[6272 1484]\n",
      " [ 529  258]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (28476, 795)\n",
      "(19933, 793)\n",
      "running xgb...\n",
      "Train AUC:  0.6795859784496009\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.53287461381206 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6324345033037504 0.5038002434442063 0.7643684888212572 0.5682457258978252 0.11047509281625503 0.3278271918678526\n",
      "[[6272 1484]\n",
      " [ 529  258]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (28476, 169)\n",
      "(19933, 167)\n",
      "running xgb...\n",
      "Train AUC:  0.8457655979417902\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.540482161156669 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6362358805053496 0.5064858984823083 0.8011237270279761 0.5582326557199149 0.10851852556543629 0.2604828462515883\n",
      "[[6639 1117]\n",
      " [ 582  205]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (28476, 903)\n",
      "(19933, 901)\n",
      "running xgb...\n",
      "Train AUC:  0.8457655979417902\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.540482161156669 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6362358805053496 0.5064858984823083 0.8011237270279761 0.5582326557199149 0.10851852556543629 0.2604828462515883\n",
      "[[6639 1117]\n",
      " [ 582  205]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (28476, 905)\n",
      "(19933, 903)\n",
      "running xgb...\n",
      "Train AUC:  0.8457655979417902\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.540482161156669 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6362358805053496 0.5064858984823083 0.8011237270279761 0.5582326557199149 0.10851852556543629 0.2604828462515883\n",
      "[[6639 1117]\n",
      " [ 582  205]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (28476, 169)\n",
      "(19933, 167)\n",
      "running xgb...\n",
      "Train AUC:  0.8457655979417902\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.540482161156669 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6362358805053496 0.5064858984823083 0.8011237270279761 0.5582326557199149 0.10851852556543629 0.2604828462515883\n",
      "[[6639 1117]\n",
      " [ 582  205]]\n",
      "length of x_df 31640\n",
      "length of xy_df 30058\n",
      "length of xy_df 28476\n",
      "imbalance target_flood_4    0.095449\n",
      "dtype: float64\n",
      "shape of xy_df with nlp features now included (28476, 903)\n",
      "(19933, 901)\n",
      "running xgb...\n",
      "Train AUC:  0.8457655979417902\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
      "maximum f1 score, thres 0.540482161156669 0.6\n",
      "auc, f1, accu, accu_bl, precision, recall=  0.6362358805053496 0.5064858984823083 0.8011237270279761 0.5582326557199149 0.10851852556543629 0.2604828462515883\n",
      "[[6639 1117]\n",
      " [ 582  205]]\n"
     ]
    }
   ],
   "source": [
    "for n_pred in horizons:\n",
    "    result_dict[n_pred] = []\n",
    "    for modality in modality_combi:\n",
    "        \n",
    "        if 'nlp' in modality:\n",
    "            for nlp_name in nlp_names:\n",
    "                result_dict[n_pred].append(data_production(df_yrly,n_pred,y_master,modality,nlp_name))\n",
    "            \n",
    "        else:\n",
    "            result_dict[n_pred].append(data_production(df_yrly,n_pred,y_master,modality,''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a5cfed7e-ea6e-4d56-b048-048a1e1ce469",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list = []\n",
    "for key in result_dict:\n",
    "    concat_list.append(pd.concat(result_dict[key],axis= 1))\n",
    "lol = pd.concat(concat_list)\n",
    "\n",
    "lol.to_csv('results_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba620557-ab14-4b90-a6da-995be91779a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
